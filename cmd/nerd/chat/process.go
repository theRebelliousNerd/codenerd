// Package chat provides the interactive TUI chat interface for codeNERD.
// This file contains input processing and intent handling.
package chat

import (
	"codenerd/internal/articulation"
	"codenerd/internal/autopoiesis"
	"codenerd/internal/campaign"
	ctxcompress "codenerd/internal/context"
	"codenerd/internal/core"
	"codenerd/internal/logging"
	"codenerd/internal/perception"
	"codenerd/internal/prompt"
	"codenerd/internal/retrieval"
	"codenerd/internal/usage"
	"codenerd/internal/world"
	"context"
	"encoding/json"
	"fmt"
	"path/filepath"
	"regexp"
	"sort"
	"strings"
	"time"

	tea "github.com/charmbracelet/bubbletea"
)

// =============================================================================
// INPUT PROCESSING
// =============================================================================

func (m Model) processInput(input string) tea.Cmd {
	return func() tea.Msg {
		// Guard: ensure transducer is initialized
		if m.transducer == nil {
			return errorMsg(fmt.Errorf("system not ready: transducer not initialized (boot may still be in progress)"))
		}
		if m.client == nil {
			return errorMsg(fmt.Errorf("system not ready: LLM client not initialized"))
		}

		ctx, cancel := context.WithTimeout(context.Background(), 30*time.Minute)
		if m.usageTracker != nil {
			ctx = usage.NewContext(ctx, m.usageTracker)
		}
		defer cancel()

		trimmed := strings.TrimSpace(input)

		// If we are waiting for clarifier answers for a future launch, just accumulate the answers.
		if m.launchClarifyPending && trimmed != "" && !strings.HasPrefix(trimmed, "/") {
			if m.launchClarifyAnswers != "" {
				m.launchClarifyAnswers += "\n"
			}
			m.launchClarifyAnswers += input
			// Continue normal processing (do not auto-start)
		}

		var warnings []string
		workspaceScanned := false

		// Baseline counts for system action facts so we can surface new ones.
		baseRoutingCount, baseExecCount := 0, 0
		if m.kernel != nil {
			if facts, err := m.kernel.Query("routing_result"); err == nil {
				baseRoutingCount = len(facts)
			}
			if facts, err := m.kernel.Query("execution_result"); err == nil {
				baseExecCount = len(facts)
			}
		}

		// =====================================================================
		// 0. FOLLOW-UP DETECTION (Pre-Perception)
		// =====================================================================
		// Check if this is a follow-up question about the last shard result.
		// This must happen BEFORE perception to inject proper context.
		isFollowUp, followUpType := detectFollowUpQuestion(input, m.lastShardResult)
		if isFollowUp && m.lastShardResult != nil {
			// Handle follow-up directly with conversation context
			return m.handleFollowUpQuestion(ctx, input, followUpType)
		}

		// 1. PERCEPTION (Transducer) - with conversation history for context
		m.ReportStatus("Perception: parsing intent...")
		// Convert history to perception.ConversationTurn format
		// Use ALL history until compression kicks in, then use recent window only
		var historyForPerception []perception.ConversationTurn
		var recentTurns []Message
		if m.compressor != nil && m.compressor.IsCompressionActive() {
			// Compression active: use recent window (compressed context handles the rest)
			recentTurns = m.getRecentTurns(m.compressor.GetRecentTurnWindow())
		} else {
			// No compression yet: pass ALL history so LLM has full context
			recentTurns = m.history
		}
		for _, msg := range recentTurns {
			historyForPerception = append(historyForPerception, perception.ConversationTurn{
				Role:    msg.Role,
				Content: msg.Content,
			})
		}

		intentHandledBySystem := false
		var intent perception.Intent
		var err error

		// Prefer the always-on PerceptionFirewall system shard when available.
		// This ensures Perception emits canonical facts (user_intent, processed_intent,
		// focus_resolution, ambiguity_flag) into the shared kernel and uses JIT prompts.
		if m.shardMgr != nil {
			if shard, ok := m.shardMgr.GetRunningShardByConfigName("perception_firewall"); ok {
				type perceiver interface {
					Perceive(ctx context.Context, input string, history []perception.ConversationTurn) (perception.Intent, error)
				}
				if pf, ok := shard.(perceiver); ok {
					intent, err = pf.Perceive(ctx, input, historyForPerception)
					intentHandledBySystem = err == nil
				}
			}
		}

		// Fallback: direct transducer parsing (legacy / when system shards are disabled).
		if !intentHandledBySystem {
			intent, err = m.transducer.ParseIntentWithContext(ctx, input, historyForPerception)
		}
		if err != nil {
			return errorMsg(fmt.Errorf("perception error: %w", err))
		}
		if strings.TrimSpace(intent.Response) == "" {
			warnings = append(warnings, "Perception response was empty; falling back to articulation")
		}
		m.ReportStatus(fmt.Sprintf("Orient: %s", intent.Verb))

		// Seed the shared kernel immediately so system shards can begin deriving actions.
		if m.kernel != nil {
			// CONTINUATION PROTOCOL: Clean up stale continuation facts from previous turns
			// This prevents old pending_test/pending_review facts from triggering false continuations
			_ = m.kernel.Retract("shard_result")
			_ = m.kernel.Retract("pending_test")
			_ = m.kernel.Retract("pending_review")
			_ = m.kernel.Retract("interrupt_requested")

			// STALE ACTION CLEANUP: Clear action pipeline facts from previous turns/sessions
			// These facts accumulate and cause misleading "System actions" displays for greetings
			// and other conversational intents that don't actually trigger shard work.
			_ = m.kernel.Retract("execution_result")
			_ = m.kernel.Retract("routing_result")
			_ = m.kernel.Retract("pending_action")
			_ = m.kernel.Retract("delegate_task")

			// Only assert user_intent ourselves if the PerceptionFirewall shard didn't already do it.
			if !intentHandledBySystem {
				// Use a stable ID so the kernel doesn't accumulate historical intents.
				intentID := "/current_intent"
				_ = m.kernel.RetractFact(core.Fact{Predicate: "user_intent", Args: []interface{}{intentID}})
				_ = m.kernel.RetractFact(core.Fact{Predicate: "processed_intent", Args: []interface{}{intentID}})
				_ = m.kernel.RetractFact(core.Fact{Predicate: "executive_processed_intent", Args: []interface{}{intentID}})
				intentFact := core.Fact{
					Predicate: "user_intent",
					Args: []interface{}{
						intentID,
						intent.Category,
						intent.Verb,
						intent.Target,
						intent.Constraint,
					},
				}
				if err := m.kernel.Assert(intentFact); err != nil {
					warnings = append(warnings, fmt.Sprintf("[Kernel] failed to assert user_intent: %v", err))
				}
				_ = m.kernel.Assert(core.Fact{Predicate: "processed_intent", Args: []interface{}{intentID}})
			}

			// If this is an issue-driven request, seed issue facts for activation and JIT selection.
			m.seedIssueFacts(intent, input)

			// GAP-002 FIX: Seed campaign facts if there's an active campaign.
			// This enables the activation engine and JIT compiler to be campaign-aware.
			m.seedCampaignFacts()
		}

		// 1.3.1 MEMORY OPERATIONS: Process promote_to_long_term, forget, etc.
		// This enables "remember X" and "learn that Y" instructions
		if len(intent.MemoryOperations) > 0 && m.localDB != nil {
			for _, memOp := range intent.MemoryOperations {
				switch memOp.Op {
				case "promote_to_long_term":
					if err := m.localDB.StoreFact(memOp.Key, []interface{}{memOp.Value}, "learned", 10); err != nil {
						warnings = append(warnings, fmt.Sprintf("[Memory] failed to store: %v", err))
					}
				case "forget":
					if m.kernel != nil {
						m.kernel.Retract(memOp.Key)
					}
				}
			}
		}

		// 1.3.2 DREAM STATE: Multi-agent simulation/learning mode
		// When user asks "what if", "imagine", "hypothetically" - consult all shards without executing
		if intent.Verb == "/dream" {
			m.ReportStatus("Dream: consulting shards...")
			return m.handleDreamState(ctx, intent, input)
		}

		// 1.3.3 ASSAULT CAMPAIGN: Auto-start adversarial assault campaigns from natural language.
		// Example: "run an assault campaign on internal/core"
		if args, ok := assaultArgsFromNaturalLanguage(m.workspace, input, intent); ok {
			m.ReportStatus("Assault: starting campaign...")
			cmd := m.startAssaultCampaign(args)
			return cmd()
		}

		// 1.4 AUTO-CLARIFICATION: If the request looks like a campaign/plan ask, run the clarifier shard
		if m.shouldAutoClarify(&intent, input) {
			m.ReportStatus("Clarifier: generating questions...")
			if res, err := m.runClarifierShard(ctx, input); err == nil && res != "" {
				surface := m.appendSystemSummary(
					res+"\n\nReply with answers, then use `/launchcampaign <goal>` when you are ready to start.",
					m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount),
				)
				return assistantMsg{
					Surface: surface,
					ClarifyUpdate: &ClarifyUpdate{
						LastClarifyInput:     input,
						LaunchClarifyPending: true,
						LaunchClarifyGoal:    input,
						LaunchClarifyAnswers: "",
					},
				}
			} else if err != nil {
				warnings = append(warnings, fmt.Sprintf("Clarifier shard unavailable: %v", err))
			}
		}

		// 1.4.1 GENERAL CLARIFICATION: Guard ambiguous intents before delegation.
		if m.shouldClarifyIntent(&intent, input) {
			m.ReportStatus("Clarifier: resolving ambiguity...")
			if res, err := m.runClarifierShard(ctx, input); err == nil && res != "" {
				return clarificationMsg{
					Question:      res,
					Options:       []string{},
					Context:       input,
					PendingIntent: &intent,
				}
			} else if err != nil {
				warnings = append(warnings, fmt.Sprintf("Clarifier shard unavailable: %v", err))
			}
		}

		// 1.5 MULTI-STEP TASK DETECTION: Check if task requires multiple steps
		// This implements autonomous multi-step execution without campaigns
		isMultiStep := detectMultiStepTask(input, intent)
		if isMultiStep {
			m.ReportStatus("Multi-step: decomposing task...")
			steps := decomposeTask(input, intent, m.workspace)
			if len(steps) > 1 {
				cmd := m.executeMultiStepTask(ctx, intent, input, steps)
				return cmd()
			}
		}

		// 1.6 DELEGATION CHECK: Route to appropriate shard if verb indicates delegation
		// This implements automatic shard spawning from natural language
		// Uses verification loop to ensure quality (no mock code, no placeholders)
		shardType := perception.GetShardTypeForVerb(intent.Verb)
		if shardType != "" && intent.Confidence >= 0.5 {
			if m.needsWorkspaceScanForDelegation(intent) && !workspaceScanned {
				workspaceScanned = m.loadWorkspaceFacts(ctx, intent, &warnings)
			}
			m.ReportStatus(fmt.Sprintf("Act: delegating to %s...", shardType))
			// Format task based on verb and target, with prior shard context (blackboard pattern)
			// This enables cross-shard context: reviewer findings -> coder, test errors -> debugger
			task := formatShardTaskWithContext(intent.Verb, intent.Target, intent.Constraint, m.workspace, m.lastShardResult)

			// Build session context for shard injection (Blackboard Pattern)
			sessionCtx := m.buildSessionContext(ctx)

			// Use verification loop if available (quality-enforcing retry)
			if m.verifier != nil {
				// Set session context for verification persistence
				m.verifier.SetSessionContext(m.sessionID, m.turnCount)

				result, verification, verifyErr := m.verifier.VerifyWithRetry(ctx, task, shardType, 3)

				// CRITICAL FIX: Inject verified shard results as facts for cross-turn context
				shardID := fmt.Sprintf("%s-verified-%d", shardType, time.Now().UnixNano())
				facts := m.shardMgr.ResultToFacts(shardID, shardType, task, result, verifyErr)
				if m.kernel != nil && len(facts) > 0 {
					_ = m.kernel.LoadFacts(facts)
				}

				srPayload := &ShardResultPayload{
					ShardType: shardType,
					Task:      task,
					Result:    result,
					Facts:     facts,
				}

				if verifyErr != nil {
					// Check if max retries exceeded - escalate to user
					if verifyErr.Error() == "max retries exceeded - escalating to user" {
						response := formatVerificationEscalation(task, shardType, verification)
						return assistantMsg{
							Surface:     m.appendSystemSummary(response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount)),
							ShardResult: srPayload,
						}
					}
					return errorMsg(fmt.Errorf("verified execution failed: %w", verifyErr))
				}

				// Format response with verification confidence
				response := formatVerifiedResponse(intent, shardType, task, result, verification)
				return assistantMsg{
					Surface:     m.appendSystemSummary(response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount)),
					ShardResult: srPayload,
				}
			}

			// Shard spawn with queue backpressure management (user-initiated = high priority)
			result, spawnErr := m.shardMgr.SpawnWithPriority(ctx, shardType, task, sessionCtx, core.PriorityHigh)

			// CRITICAL FIX: Inject shard results as facts for cross-turn context
			// This enables the main agent to reference shard outputs in future turns
			shardID := fmt.Sprintf("%s-%d", shardType, time.Now().UnixNano())
			facts := m.shardMgr.ResultToFacts(shardID, shardType, task, result, spawnErr)
			if m.kernel != nil && len(facts) > 0 {
				if loadErr := m.kernel.LoadFacts(facts); loadErr != nil {
					warnings = append(warnings, fmt.Sprintf("[ShardFacts] Warning: %v", loadErr))
				}
			}

			srPayload := &ShardResultPayload{
				ShardType: shardType,
				Task:      task,
				Result:    result,
				Facts:     facts,
			}

			if spawnErr != nil {
				return errorMsg(fmt.Errorf("shard delegation failed: %w", spawnErr))
			}

			// 1.6.1 AUTO-INTERPRETATION: For Reviewer/Tester shards, explain the findings
			if shardType == "reviewer" || shardType == "tester" {
				// 1.6.2 REVIEWER VALIDATION CHECK: Self-correction feedback loop
				// Check if the review shows signs of inaccuracy and flag for user attention
				var validationWarning string
				if shardType == "reviewer" {
					if m.shardMgr.CheckReviewNeedsValidation(shardID) {
						reasons := m.shardMgr.GetReviewSuspectReasons(shardID)
						if len(reasons) > 0 {
							validationWarning = "**Review Validation Alert**: This review may contain inaccuracies.\n"
							validationWarning += "Reasons: " + strings.Join(reasons, ", ") + "\n"
							validationWarning += "Please verify findings before acting on them. "
							validationWarning += "Use `/reject-finding <file>:<line> <reason>` to help the system learn from mistakes."
							warnings = append(warnings, "Review flagged for validation: "+strings.Join(reasons, ", "))
						}
					}
				}

				response := m.formatInterpretedResult(ctx, input, shardType, task, result, validationWarning)
				surface := m.appendSystemSummary(response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount))

				// CONTINUATION PROTOCOL: Check for pending subtasks before returning
				if cont := m.checkContinuation(shardType, task, result); cont != nil {
					return continuationInitMsg{
						completedSurface: surface,
						firstResult:      srPayload,
						next:             *cont,
						totalSteps:       0,
					}
				}

				return assistantMsg{
					Surface:     surface,
					ShardResult: srPayload,
				}
			}

			// Format a rich response combining LLM surface response and shard result
			response := formatDelegatedResponse(intent, shardType, task, result)
			surface := m.appendSystemSummary(response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount))

			// CONTINUATION PROTOCOL: Check for pending subtasks before returning
			if cont := m.checkContinuation(shardType, task, result); cont != nil {
				return continuationInitMsg{
					completedSurface: surface,
					firstResult:      srPayload,
					next:             *cont,
					totalSteps:       0,
				}
			}

			return assistantMsg{
				Surface:     surface,
				ShardResult: srPayload,
			}
		}

		// 1.7 STATS: Deterministic codebase/file statistics (no LLM follow-up).
		if shardType == "" && intent.Verb == "/stats" {
			m.ReportStatus("Stats: computing...")
			statsResp, err := m.handleStatsIntent(ctx, intent)
			if err != nil {
				return responseMsg(m.appendSystemSummary(
					fmt.Sprintf("Stats error: %v", err),
					m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount),
				))
			}
			return responseMsg(m.appendSystemSummary(statsResp, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount)))
		}

		// 1.7 DIRECT RESPONSE: For non-actionable verbs (/explain, /read, etc.) with
		// no shard and a valid perception response, return the perception response
		// directly. This handles greetings, capability questions, and general queries
		// without requiring a second articulation LLM call.
		if shardType == "" && intent.Response != "" && isConversationalIntent(intent) {
			return responseMsg(m.appendSystemSummary(intent.Response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount)))
		}

		// 1.8 AUTOPOIESIS CHECK: Analyze for complexity, persistence, and tool needs
		// This implements ¬ß8.3: Self-modification capabilities
		if m.autopoiesis != nil {
			autoResult := m.autopoiesis.QuickAnalyze(ctx, input, intent.Target)

			// Auto-trigger campaign for complex tasks
			if autoResult.NeedsCampaign && autoResult.ComplexityLevel >= autopoiesis.ComplexityComplex {
				needsCampaign, reason := m.autopoiesis.ShouldTriggerCampaign(ctx, input, intent.Target)
				if needsCampaign && m.activeCampaign == nil {
					warnings = append(warnings, fmt.Sprintf("Complex task detected: %s", reason))
					warnings = append(warnings, "Consider using `/campaign start` for multi-phase execution")
					warnings = append(warnings, "Use `/clarify <goal>` to gather requirements before starting the campaign")

					// Automatically run the Requirements Interrogator once to elicit details
					if clarifierMsg, err := m.runClarifierShard(ctx, input); err == nil && clarifierMsg != "" {
						// Return immediately with clarifying questions; user can answer then start campaign
						return responseMsg(m.appendSystemSummary(
							clarifierMsg+"\n\nReply with answers, then run `/campaign start <goal>` to kick off the plan.",
							m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount),
						))
					} else if err != nil {
						warnings = append(warnings, fmt.Sprintf("Clarifier shard unavailable: %v", err))
					}
				}
			}

			// Check for persistent agent needs
			if autoResult.NeedsPersistent {
				needsPersist, persistNeed := m.autopoiesis.ShouldCreatePersistentAgent(ctx, input)
				if needsPersist && persistNeed != nil {
					warnings = append(warnings, fmt.Sprintf("Persistent agent recommended: %s (%s)", persistNeed.AgentType, persistNeed.Purpose))
					warnings = append(warnings, "Use `/define-agent` to create a persistent specialist")
				}
			}
		}

		// 2. CONTEXT LOADING (Scanner)
		// Load workspace facts only if intent requires it (optimization).
		// Use incremental scan to avoid reparsing unchanged repos.
		if !workspaceScanned {
			workspaceScanned = m.loadWorkspaceFacts(ctx, intent, &warnings)
		}

		// 3. STATE UPDATE (Kernel)
		// user_intent was asserted earlier; now refresh system facts and shard facts.
		// Fix Bug #7: Update system facts (Time, etc.)
		if err := m.kernel.UpdateSystemFacts(); err != nil {
			return errorMsg(fmt.Errorf("system facts update error: %w", err))
		}
		_ = m.kernel.LoadFacts(m.shardMgr.ToFacts())
		// Hydrate learned facts from knowledge.db so logic can use persisted context
		if m.virtualStore != nil {
			if _, err := m.virtualStore.HydrateLearnings(ctx); err != nil {
				warnings = append(warnings, fmt.Sprintf("Hydrate learnings warning: %v", err))
			}
			if _, err := m.virtualStore.HydrateSessionContext(ctx, m.sessionID, input, m.collectTraceShardTypes()); err != nil {
				warnings = append(warnings, fmt.Sprintf("Hydrate session context warning: %v", err))
			}
		}

		// 4. DECISION & ACTION (Kernel -> Executor)
		// Query for actions derived from the intent
		actions, _ := m.kernel.Query("next_action")

		// Execute Info-Gathering Actions (Pre-Articulation)
		// This implements the OODA "Act" phase for info retrieval
		var executionResults []core.Fact
		var mangleUpdates []string

		for _, action := range actions {
			actionName := nextActionName(action)
			if actionName != "" {
				mangleUpdates = append(mangleUpdates, actionName)
			}
			actionKey := normalizeActionType(actionName)

			// Handle File System Reads
			if actionKey == "fs_read" {
				target := intent.Target // Simple mapping for now
				if target != "" && target != "none" {
					content, err := readFileContent(m.workspace, target, 8000)
					if err == nil {
						// Feed result back to kernel
						resFact := core.Fact{
							Predicate: "file_content",
							Args:      []interface{}{target, content},
						}
						executionResults = append(executionResults, resFact)
						// Also allow articulation to see it
						warnings = append(warnings, fmt.Sprintf("Read file: %s (%d bytes)", target, len(content)))
					} else {
						warnings = append(warnings, fmt.Sprintf("Failed to read file %s: %v", target, err))
					}
				}
			}

			// Handle Search
			if actionKey == "search_files" {
				matches, err := searchInFiles(m.workspace, intent.Target, 10)
				if err == nil {
					resFact := core.Fact{
						Predicate: "search_results",
						Args:      []interface{}{intent.Target, strings.Join(matches, ",")},
					}
					executionResults = append(executionResults, resFact)
					warnings = append(warnings, fmt.Sprintf("Found %d matches for '%s'", len(matches), intent.Target))
				}
			}

			// Autopoiesis: Tool Generation (¬ß8.3)
			if actionKey == "generate_tool" && m.autopoiesis != nil {
				// Detect tool need from the input
				toolNeed, detectErr := m.autopoiesis.DetectToolNeed(ctx, input)
				if detectErr == nil && toolNeed != nil {
					warnings = append(warnings, fmt.Sprintf("Tool need detected: %s (confidence: %.2f)", toolNeed.Name, toolNeed.Confidence))

					// Generate the tool if confidence is high enough
					if toolNeed.Confidence >= 0.6 {
						genTool, genErr := m.autopoiesis.GenerateTool(ctx, toolNeed)
						if genErr == nil && genTool != nil {
							warnings = append(warnings, fmt.Sprintf("Generated tool: %s", genTool.Name))
							if genTool.Validated {
								warnings = append(warnings, "Tool code validated successfully")
							} else if len(genTool.Errors) > 0 {
								warnings = append(warnings, fmt.Sprintf("Tool validation warnings: %v", genTool.Errors))
							}
						} else if genErr != nil {
							warnings = append(warnings, fmt.Sprintf("Tool generation failed: %v", genErr))
						}
					} else {
						warnings = append(warnings, "Tool need confidence too low for auto-generation")
					}
				} else {
					warnings = append(warnings, "Autopoiesis: Analyzing for missing tool capabilities...")
				}
			}
		}

		// Feed execution results back into kernel for re-evaluation
		if len(executionResults) > 0 {
			_ = m.kernel.LoadFacts(executionResults)
			// Re-query context to inject (now that we have new facts)
		}

		// 4.5 SYSTEM ACTION HANDLING: Surface kernel-driven delegations and execution results.
		if msg := m.handleSystemDelegations(ctx, input, intent, baseRoutingCount, baseExecCount); msg != nil {
			return msg
		}

		// 5. CONTEXT SELECTION (Spreading Activation)
		contextFacts, _ := m.kernel.Query("context_to_inject")

		// 6. ARTICULATION (Response Generation)
		systemPrompts, _ := m.kernel.Query("final_system_prompt")
		systemPrompt := ""
		if len(systemPrompts) > 0 && len(systemPrompts[0].Args) > 0 {
			systemPrompt = fmt.Sprintf("%v", systemPrompts[0].Args[0])
		}

		// Inject the "Steven Moore Flare" persona
		systemPrompt += "\n\n" + stevenMoorePersona
		// Build conversation context for fluid chat experience
		// This enables the LLM to understand recent turns and reference previous outputs
		// Now includes compressed session context (Blackboard Pattern + Infinite Context)
		var compressedCtx string
		var recentTurnsForArticulation []Message
		if m.compressor != nil && m.compressor.IsCompressionActive() {
			// Compression active: use recent window + compressed context for older turns
			if ctxStr, err := m.compressor.GetContextString(ctx); err == nil {
				compressedCtx = ctxStr
			}
			recentTurnsForArticulation = m.getRecentTurns(m.compressor.GetRecentTurnWindow())
		} else {
			// No compression yet: pass ALL history so LLM has full context
			recentTurnsForArticulation = m.history
		}
		convCtx := &ConversationContext{
			RecentTurns:     recentTurnsForArticulation,
			LastShardResult: m.lastShardResult,
			TurnNumber:      m.turnCount,
			ShardHistory:    m.shardResultHistory, // Blackboard: all recent shard results
			CompressedCtx:   compressedCtx,        // Infinite context: compressed session (empty if not active)
		}

		// Use full articulation output to capture MemoryOperations
		// Now with conversation context for fluid follow-up handling
		artOutput, err := articulateWithConversation(ctx, m.client, intent, payloadForArticulation(intent, mangleUpdates), contextFacts, warnings, systemPrompt, convCtx)
		if err != nil {
			return errorMsg(err)
		}

		response := artOutput.Surface

		// Add any articulation warnings to the flow
		if len(artOutput.Warnings) > 0 {
			for _, w := range artOutput.Warnings {
				warnings = append(warnings, fmt.Sprintf("[Articulation] %s", w))
			}
		}

		// 7. SEMANTIC COMPRESSION (Process turn for infinite context)
		// This implements ¬ß8.2: Compress surface text, retain only logical atoms
		// Now properly wired with MemoryOperations from articulation!
		if m.compressor != nil {
			// Convert articulation.MemoryOperation to perception.MemoryOperation
			var memOps []perception.MemoryOperation
			for _, op := range artOutput.MemoryOperations {
				memOps = append(memOps, perception.MemoryOperation{
					Op:    op.Op,
					Key:   op.Key,
					Value: op.Value,
				})
			}

			// Merge mangle updates from articulation with pre-existing ones
			allMangleUpdates := mangleUpdates
			if len(artOutput.MangleUpdates) > 0 {
				allMangleUpdates = append(allMangleUpdates, artOutput.MangleUpdates...)

				// STRATIFIED TRUST (Bug #15 Fix): Validate learned facts
				// Learned facts must be proposed as candidate_action() and validated
				var newFacts []core.Fact
				var learnedFacts []core.Fact

				for _, s := range artOutput.MangleUpdates {
					if f, err := core.ParseSingleFact(s); err == nil {
						// Check if this is a learned action proposal
						if strings.HasPrefix(f.Predicate, "candidate_action") {
							learnedFacts = append(learnedFacts, f)
						} else {
							// System-level facts are loaded directly
							newFacts = append(newFacts, f)
						}
					}
				}

				// Load system facts immediately
				if len(newFacts) > 0 {
					_ = m.kernel.LoadFacts(newFacts)
				}

				// Learned facts go through validation
				if len(learnedFacts) > 0 {
					// Load candidate proposals
					_ = m.kernel.LoadFacts(learnedFacts)

					// Query for final_action (validated by Constitution)
					validatedActions, _ := m.kernel.Query("final_action")
					if len(validatedActions) > 0 {
						warnings = append(warnings, fmt.Sprintf("[Stratified Trust] %d learned actions validated", len(validatedActions)))
					}

					// Query for denied actions (audit trail)
					deniedActions, _ := m.kernel.Query("action_denied")
					if len(deniedActions) > 0 {
						warnings = append(warnings, fmt.Sprintf("[Stratified Trust] %d learned actions BLOCKED by Constitution", len(deniedActions)))
					}
				}
			}

			controlPacket := &perception.ControlPacket{
				IntentClassification: perception.IntentClassification{
					Category:   intent.Category,
					Verb:       intent.Verb,
					Target:     intent.Target,
					Constraint: intent.Constraint,
					Confidence: intent.Confidence,
				},
				MangleUpdates:    allMangleUpdates,
				MemoryOperations: memOps, // Now properly populated from articulation!
			}

			// Handle self-correction if triggered
			if artOutput.SelfCorrection != nil && artOutput.SelfCorrection.Triggered {
				controlPacket.SelfCorrection = &perception.SelfCorrection{
					Triggered:  true,
					Hypothesis: artOutput.SelfCorrection.Hypothesis,
				}
			}

			turn := ctxcompress.Turn{
				Number:          m.turnCount, // zero-based turn numbering
				Role:            "assistant",
				UserInput:       input,
				SurfaceResponse: response,
				ControlPacket:   controlPacket,
				Timestamp:       time.Now(),
			}

			// Process turn asynchronously - don't block response
			go func(t ctxcompress.Turn, capturedIntent perception.Intent, capturedResponse string) {
				// Use a shutdown-scoped context so compression can finish after the main turn ctx is canceled.
				baseCtx := m.shutdownCtx
				if baseCtx == nil {
					baseCtx = context.Background()
				}
				compressCtx, cancel := context.WithTimeout(baseCtx, 2*time.Minute)
				defer cancel()
				// COMPRESSION: Semantic compression for infinite context (¬ß8.2)
				if _, err := m.compressor.ProcessTurn(compressCtx, t); err != nil {
					// Log compression errors but don't fail the turn
					fmt.Printf("[Compressor] Warning: %v\n", err)
				}

				// KNOWLEDGE PERSISTENCE: Populate knowledge.db tables for learning
				// This implements the missing learning loop identified in user feedback
				if m.localDB != nil {
					m.persistTurnToKnowledge(t, capturedIntent, capturedResponse)
				}
			}(turn, intent, response)
		}

		return responseMsg(m.appendSystemSummary(response, m.collectSystemSummary(ctx, baseRoutingCount, baseExecCount)))
	}
}

// seedIssueFacts extracts issue text + keywords from user input and asserts issue_* facts.
// This drives issue-aware spreading activation and prompt atom selection.
func (m *Model) seedIssueFacts(intent perception.Intent, rawInput string) {
	if m.kernel == nil {
		return
	}

	// Only seed for verbs that are typically issue-driven.
	switch intent.Verb {
	case "/fix", "/debug", "/review", "/security":
	default:
		return
	}

	issueText := strings.TrimSpace(rawInput)
	if issueText == "" {
		return
	}

	// Keep the stored issue text bounded to avoid EDB bloat.
	const maxIssueChars = 4000
	if len(issueText) > maxIssueChars {
		issueText = issueText[:maxIssueChars]
	}

	issueID := fmt.Sprintf("/issue_%d", time.Now().UnixNano())
	keywords := retrieval.ExtractKeywords(issueText)

	facts := make([]core.Fact, 0, 1+len(keywords.Weights)+len(keywords.MentionedFiles))
	facts = append(facts, core.Fact{
		Predicate: "issue_text",
		Args:      []interface{}{issueID, issueText},
	})

	for kw, weight := range keywords.Weights {
		if strings.TrimSpace(kw) == "" {
			continue
		}
		facts = append(facts, core.Fact{
			Predicate: "issue_keyword",
			Args:      []interface{}{issueID, kw, weight},
		})
	}

	for _, file := range keywords.MentionedFiles {
		if strings.TrimSpace(file) == "" {
			continue
		}
		facts = append(facts, core.Fact{
			Predicate: "file_mentioned",
			Args:      []interface{}{file, issueID},
		})
	}

	// GAP-017 FIX: Assert tiered_context_file facts for issue-driven file relevance
	// Tier 1: Directly mentioned files (highest relevance)
	// This enables the activation engine to boost these files in context selection
	for i, file := range keywords.MentionedFiles {
		if strings.TrimSpace(file) == "" {
			continue
		}
		// tiered_context_file(IssueID, File, Tier, Relevance, TokenCount)
		// Tier 1 for directly mentioned, relevance decreases by position
		relevance := 1.0 - (float64(i) * 0.1)
		if relevance < 0.5 {
			relevance = 0.5
		}
		facts = append(facts, core.Fact{
			Predicate: "tiered_context_file",
			Args:      []interface{}{issueID, file, "/tier1", relevance, 0},
		})
	}

	_ = m.kernel.LoadFacts(facts)
}

// seedCampaignFacts asserts campaign context facts for spreading activation and JIT selection.
// GAP-002 FIX: This enables the activation engine and JIT compiler to be campaign-aware.
func (m *Model) seedCampaignFacts() {
	if m.kernel == nil || m.activeCampaign == nil {
		return
	}

	c := m.activeCampaign
	facts := make([]core.Fact, 0, 10)

	// current_campaign(CampaignID)
	facts = append(facts, core.Fact{
		Predicate: "current_campaign",
		Args:      []interface{}{c.ID},
	})

	// Find current phase (first non-completed phase)
	var currentPhase *campaign.Phase
	for i := range c.Phases {
		if c.Phases[i].Status != campaign.PhaseCompleted {
			currentPhase = &c.Phases[i]
			break
		}
	}

	if currentPhase != nil {
		// current_phase(PhaseID)
		facts = append(facts, core.Fact{
			Predicate: "current_phase",
			Args:      []interface{}{currentPhase.ID},
		})

		// phase_objective(PhaseID, ObjectiveIndex, Description)
		for i, obj := range currentPhase.Objectives {
			objID := fmt.Sprintf("/obj_%s_%d", currentPhase.ID, i)
			facts = append(facts, core.Fact{
				Predicate: "phase_objective",
				Args:      []interface{}{currentPhase.ID, objID, obj.Description},
			})
		}

		// Find next pending task
		for _, task := range currentPhase.Tasks {
			if task.Status == campaign.TaskPending || task.Status == campaign.TaskInProgress {
				// next_campaign_task(TaskID)
				facts = append(facts, core.Fact{
					Predicate: "next_campaign_task",
					Args:      []interface{}{task.ID},
				})

				// task_artifact(TaskID, ArtifactType, Path)
				for _, artifact := range task.Artifacts {
					facts = append(facts, core.Fact{
						Predicate: "task_artifact",
						Args:      []interface{}{task.ID, artifact.Type, artifact.Path},
					})
				}
				break // Only the next task
			}
		}
	}

	_ = m.kernel.LoadFacts(facts)
}

// runClarifierShard invokes the requirements_interrogator shard synchronously to gather clarifying questions.
func (m Model) runClarifierShard(ctx context.Context, goal string) (string, error) {
	if m.shardMgr == nil {
		return "", fmt.Errorf("shard manager not initialized")
	}

	cctx, cancel := context.WithTimeout(ctx, 90*time.Second)
	defer cancel()

	result, err := m.shardMgr.Spawn(cctx, "requirements_interrogator", goal)
	if err != nil {
		return "", err
	}
	return result, nil
}

// =============================================================================
// DREAM STATE - Multi-Agent Simulation/Learning Mode
// =============================================================================

// DreamConsultation holds a shard's perspective on a hypothetical task.
type DreamConsultation struct {
	ShardName   string // e.g., "coder", "my-go-expert"
	ShardType   string // e.g., "ephemeral", "persistent", "system"
	Perspective string
	Tools       []string
	Concerns    []string
	Error       error
}

// containsWord checks if text contains keyword as a whole word (not substring).
// For short keywords (‚â§3 chars), requires word boundaries.
// For longer keywords, substring match is sufficient.
func containsWord(text, keyword string) bool {
	if len(keyword) <= 3 {
		// Short keyword - need word boundary check
		// Use regex with word boundaries
		pattern := `\b` + regexp.QuoteMeta(keyword) + `\b`
		matched, _ := regexp.MatchString(pattern, text)
		return matched
	}
	// Longer keywords - substring is fine
	return strings.Contains(text, keyword)
}

// isShardRelevantToTopic checks if a specialist shard is relevant to the given topic.
// Generic shards (coder, tester, reviewer, researcher) are always relevant.
// Specialist shards are only relevant if their domain matches the topic.
func isShardRelevantToTopic(shardName string, topic string) bool {
	lower := strings.ToLower(topic)
	shardLower := strings.ToLower(shardName)

	// Generic shards are always relevant
	genericShards := []string{"coder", "tester", "reviewer", "researcher", "security", "planner"}
	for _, g := range genericShards {
		if strings.Contains(shardLower, g) {
			return true
		}
	}

	// Domain-specific keyword mapping
	domainKeywords := map[string][]string{
		"go":        {"go", "golang", "gin", "echo", "fiber", "cobra", "viper", "bubbletea", "lipgloss", "rod", "chromedp"},
		"rod":       {"browser", "automation", "scrape", "scraping", "chromedp", "puppeteer", "selenium", "web driver", "headless"},
		"mangle":    {"mangle", "datalog", "logic", "predicate", "rule", "query", "facts", "kernel"},
		"bubbletea": {"tui", "terminal", "cli", "bubbletea", "bubbles", "charm", "lipgloss", "interactive"},
		"cobra":     {"cli", "command", "flag", "subcommand", "cobra", "viper", "config"},
		"react":     {"react", "jsx", "component", "hook", "frontend", "next.js", "nextjs"},
		"vue":       {"vue", "vuex", "nuxt", "component", "frontend"},
		"python":    {"python", "pip", "django", "flask", "fastapi", "pandas", "numpy"},
		"rust":      {"rust", "cargo", "tokio", "async", "ownership", "borrow"},
		"test":      {"test", "testing", "spec", "coverage", "mock", "stub", "assert"},
		"security":  {"security", "audit", "vulnerability", "owasp", "injection", "xss", "csrf"},
	}

	// Check if shard name contains a domain keyword
	for domain, keywords := range domainKeywords {
		if strings.Contains(shardLower, domain) {
			// This is a domain specialist - check if topic matches ANY of its keywords
			for _, kw := range keywords {
				if containsWord(lower, kw) {
					return true // Topic matches this specialist's domain
				}
			}
			// Topic doesn't match this specialist's domain - skip it
			return false
		}
	}

	// Unknown specialist - include by default (might be user-defined)
	return true
}

// handleDreamState implements the "what if" simulation mode.
// It consults ALL available shards (Type A ephemeral, Type B/U persistent specialists,
// and selected Type S system shards) in parallel WITHOUT executing anything,
// aggregates their perspectives, and presents a comprehensive plan for human-in-the-loop learning.
func (m Model) handleDreamState(ctx context.Context, intent perception.Intent, input string) tea.Msg {
	if m.shardMgr == nil {
		return errorMsg(fmt.Errorf("dream state requires shard manager"))
	}

	hypothetical := intent.Target
	if hypothetical == "" {
		hypothetical = input
	}

	// Get ALL available shards dynamically (Type A, B, U, and selected S)
	availableShards := m.shardMgr.ListAvailableShards()

	// DEBUG: Log all discovered shards
	logging.Dream("Discovered %d available shards", len(availableShards))
	for i, shard := range availableShards {
		logging.Dream("  [%d] Name: %s, Type: %s, HasKnowledge: %v", i+1, shard.Name, shard.Type, shard.HasKnowledge)
	}

	// Filter shards to consult - include all except low-level system internals
	skipShards := map[string]bool{
		"perception_firewall":  true, // Internal routing - not useful to consult
		"tactile_router":       true, // Internal routing - not useful to consult
		"world_model_ingestor": true, // Background service - not useful to consult
	}

	// Sort shards: persistent specialists FIRST, then ephemeral, then system
	// This prioritizes domain experts from agents.json before generalists
	type shardPriority struct {
		name         string
		shardType    core.ShardType
		hasKnowledge bool
		priority     int // Lower = higher priority
	}

	var prioritizedShards []shardPriority
	var shardDescriptions = make(map[string]string)

	for _, shard := range availableShards {
		if skipShards[shard.Name] {
			logging.Dream("Skipping internal shard: %s", shard.Name)
			continue
		}

		// Skip specialists that aren't relevant to this topic
		if shard.HasKnowledge && !isShardRelevantToTopic(shard.Name, hypothetical) {
			logging.Dream("Skipping irrelevant specialist: %s (topic: %s)", shard.Name, hypothetical)
			continue
		}

		// Calculate priority (lower = consulted first)
		priority := 100
		switch shard.Type {
		case core.ShardTypePersistent:
			if shard.HasKnowledge {
				priority = 1 // Persistent specialists with knowledge = highest priority
			} else {
				priority = 2 // Persistent without knowledge
			}
		case core.ShardTypeUser:
			if shard.HasKnowledge {
				priority = 3 // User-defined specialists with knowledge
			} else {
				priority = 4 // User-defined without knowledge
			}
		case core.ShardTypeEphemeral:
			priority = 10 // Generalist ephemeral shards
		case core.ShardTypeSystem:
			priority = 20 // System shards last
		}

		prioritizedShards = append(prioritizedShards, shardPriority{
			name:         shard.Name,
			shardType:    shard.Type,
			hasKnowledge: shard.HasKnowledge,
			priority:     priority,
		})

		// Build description for prompt context
		typeLabel := string(shard.Type)
		if shard.HasKnowledge {
			typeLabel += " with domain knowledge"
		}
		shardDescriptions[shard.Name] = typeLabel
	}

	// Sort by priority (lower first)
	sort.Slice(prioritizedShards, func(i, j int) bool {
		return prioritizedShards[i].priority < prioritizedShards[j].priority
	})

	// Extract sorted names
	var shardTypes []string
	for _, s := range prioritizedShards {
		shardTypes = append(shardTypes, s.name)
	}

	// DEBUG: Log shards that will be consulted (in priority order)
	logging.Dream("Will consult %d shards (priority ordered - specialists first)", len(shardTypes))
	for i, s := range prioritizedShards {
		knowledgeTag := ""
		if s.hasKnowledge {
			knowledgeTag = " ‚òÖ"
		}
		logging.Dream("  [%d] %s (%s, priority=%d)%s", i+1, s.name, s.shardType, s.priority, knowledgeTag)
	}

	// Fallback to core shards if none found
	if len(shardTypes) == 0 {
		shardTypes = []string{"coder", "tester", "reviewer", "researcher"}
	}

	consultPromptTemplate := `DREAM STATE CONSULTATION - DO NOT EXECUTE ANYTHING

You are being consulted about a HYPOTHETICAL task. The user wants to understand how you would approach this WITHOUT actually doing it.

Hypothetical Task: %s

As the %s agent, provide your perspective:

1. **Your Role**: What would you specifically handle?
2. **Steps You'd Take**: Numbered list of actions (but don't do them)
3. **Tools You'd Use**: What existing tools/commands would you need?
4. **Tools You'd Need Created**: What tools don't exist that you'd want?
5. **Dependencies**: What would you need from other agents first?
6. **Risks/Concerns**: What could go wrong?
7. **Questions**: What clarifications would you need from the user?

Remember: This is a SIMULATION. Describe what you WOULD do, not what you ARE doing.
Format your response as a structured analysis.`

	// Rate limit: ~1.6 API calls per second max - process SEQUENTIALLY
	// 1 second between shards, each shard's LLM call has 600ms minimum spacing
	logging.Dream("Rate limiting: processing shards sequentially (1s delay between)")

	// Longer timeout for sequential processing (30s per shard max)
	consultCtx, cancel := context.WithTimeout(ctx, time.Duration(len(shardTypes)*30)*time.Second)
	defer cancel()

	consultations := make([]DreamConsultation, 0, len(shardTypes))

	// Track specialist responses for early stopping
	specialistResponded := false
	specialistResponseQuality := 0 // Sum of response lengths from specialists

	for i, shardName := range shardTypes {
		// Check if context cancelled
		if consultCtx.Err() != nil {
			logging.Dream("Context cancelled, stopping at shard %d/%d", i, len(shardTypes))
			break
		}

		// Rate limit: wait 1 second between shard spawns (after first one)
		if i > 0 {
			time.Sleep(1 * time.Second)
		}

		name := shardName
		typeDesc := shardDescriptions[name]
		isSpecialist := strings.Contains(typeDesc, "with domain knowledge")

		logging.Dream("[%d/%d] Consulting shard: %s (%s)",
			i+1, len(shardTypes), name, typeDesc)

		prompt := fmt.Sprintf(consultPromptTemplate, hypothetical, name)

		// Pass DreamMode=true so shards know NOT to execute, only describe
		dreamCtx := &core.SessionContext{
			DreamMode: true,
		}
		// Dream mode = low priority (background speculation)
		result, err := m.shardMgr.SpawnWithPriority(consultCtx, name, prompt, dreamCtx, core.PriorityLow)

		consultation := DreamConsultation{
			ShardName: name,
			ShardType: typeDesc,
			Error:     err,
		}

		if err == nil {
			consultation.Perspective = result
			consultation.Tools = extractToolMentions(result)
			consultation.Concerns = extractConcerns(result)
			logging.Dream("‚úì Shard %s responded (%d chars)", name, len(result))

			// Track specialist response quality
			if isSpecialist && len(result) > 200 {
				specialistResponded = true
				specialistResponseQuality += len(result)
			}
		} else {
			logging.Dream("‚úó Shard %s failed: %v", name, err)
		}

		consultations = append(consultations, consultation)

		// Early stopping: if we have substantial specialist responses, skip remaining shards
		// A good specialist response (>1000 chars total) is sufficient - don't need generalists
		if specialistResponded && specialistResponseQuality > 1000 && i < len(shardTypes)-1 {
			remainingShards := len(shardTypes) - i - 1
			// Only skip if remaining are lower priority (generalists/system)
			// Check if we've processed all specialists
			allRemainingAreGeneralists := true
			for j := i + 1; j < len(shardTypes); j++ {
				desc := shardDescriptions[shardTypes[j]]
				if strings.Contains(desc, "with domain knowledge") {
					allRemainingAreGeneralists = false
					break
				}
			}
			if allRemainingAreGeneralists {
				logging.Dream("‚ö° Early stopping: specialist(s) provided confident answer (%d chars). Skipping %d remaining generalist shards.",
					specialistResponseQuality, remainingShards)
				break
			}
		}
	}

	// DEBUG: Summary of collected consultations
	logging.Dream("Collected %d consultations", len(consultations))
	successCount := 0
	failCount := 0
	for _, c := range consultations {
		if c.Error != nil {
			failCount++
			logging.Dream("  ‚úó %s (%s): ERROR - %v", c.ShardName, c.ShardType, c.Error)
		} else {
			successCount++
			logging.Dream("  ‚úì %s (%s): OK (%d chars)", c.ShardName, c.ShardType, len(c.Perspective))
		}
	}
	logging.Dream("Summary: %d success, %d failed", successCount, failCount)

	// Aggregate and format the dream state response
	response := formatDreamStateResponse(hypothetical, consultations)

	// Store dream context for learning follow-up
	if m.kernel != nil {
		dreamFact := core.Fact{
			Predicate: "dream_state",
			Args:      []interface{}{hypothetical, time.Now().Unix()},
		}
		_ = m.kernel.Assert(dreamFact)
	}

	// Extract learnings from consultations (¬ß8.3.1 Dream Learning)
	if m.dreamCollector != nil {
		// Convert to core.DreamConsultation type
		coreConsultations := make([]core.DreamConsultation, len(consultations))
		for i, c := range consultations {
			coreConsultations[i] = core.DreamConsultation{
				ShardName:   c.ShardName,
				ShardType:   c.ShardType,
				Perspective: c.Perspective,
				Tools:       c.Tools,
				Concerns:    c.Concerns,
				Error:       c.Error,
			}
		}
		learnings := m.dreamCollector.ExtractLearnings(hypothetical, coreConsultations)
		if len(learnings) > 0 {
			logging.Dream("Extracted %d learnable insights, staged for user confirmation", len(learnings))
		}
	}

	return assistantMsg{
		Surface:           response,
		DreamHypothetical: hypothetical,
	}
}

// formatDreamStateResponse aggregates shard consultations into a structured response.
func formatDreamStateResponse(hypothetical string, consultations []DreamConsultation) string {
	var sb strings.Builder

	sb.WriteString("# üåô Dream State Analysis\n\n")
	sb.WriteString(fmt.Sprintf("**Hypothetical:** %s\n\n", hypothetical))

	// Show which shards were consulted
	sb.WriteString("**Agents Consulted:** ")
	for i, c := range consultations {
		if i > 0 {
			sb.WriteString(", ")
		}
		sb.WriteString(c.ShardName)
	}
	sb.WriteString("\n\n---\n\n")

	// Collect all unique tools and concerns
	allTools := make(map[string]bool)
	allMissingTools := make(map[string]bool)
	allConcerns := make(map[string]bool)

	// Group by shard type for organized output
	typeOrder := []string{"ephemeral", "persistent", "user", "system"}
	typeLabels := map[string]string{
		"ephemeral":  "üîÑ Type A - Ephemeral Agents (Generalists)",
		"persistent": "üíæ Type B - Persistent Specialists (Domain Experts)",
		"user":       "üë§ Type U - User-Defined Specialists",
		"system":     "‚öôÔ∏è Type S - System Agents (Policy/Safety)",
	}

	for _, shardType := range typeOrder {
		// Find consultations of this type
		var typeConsultations []DreamConsultation
		for _, c := range consultations {
			if strings.Contains(c.ShardType, shardType) || (shardType == "ephemeral" && c.ShardType == "") {
				typeConsultations = append(typeConsultations, c)
			}
		}

		if len(typeConsultations) == 0 {
			continue
		}

		sb.WriteString(fmt.Sprintf("## %s\n\n", typeLabels[shardType]))

		for _, c := range typeConsultations {
			if c.Error != nil {
				sb.WriteString(fmt.Sprintf("### %s\n\n", strings.Title(c.ShardName)))
				sb.WriteString(fmt.Sprintf("*Consultation failed: %v*\n\n", c.Error))
				continue
			}

			sb.WriteString(fmt.Sprintf("### %s\n\n", strings.Title(c.ShardName)))
			sb.WriteString(c.Perspective)
			sb.WriteString("\n\n")

			// Aggregate tools
			for _, tool := range c.Tools {
				if strings.Contains(strings.ToLower(tool), "need") || strings.Contains(strings.ToLower(tool), "create") {
					allMissingTools[tool] = true
				} else {
					allTools[tool] = true
				}
			}

			// Aggregate concerns
			for _, concern := range c.Concerns {
				allConcerns[concern] = true
			}
		}

		sb.WriteString("---\n\n")
	}

	// Summary section
	sb.WriteString("## üìã Aggregated Summary\n\n")

	if len(allTools) > 0 {
		sb.WriteString("### Existing Tools Required\n")
		for tool := range allTools {
			sb.WriteString(fmt.Sprintf("- %s\n", tool))
		}
		sb.WriteString("\n")
	}

	if len(allMissingTools) > 0 {
		sb.WriteString("### üîß Tools to Create (Autopoiesis Candidates)\n")
		for tool := range allMissingTools {
			sb.WriteString(fmt.Sprintf("- %s\n", tool))
		}
		sb.WriteString("\n")
	}

	if len(allConcerns) > 0 {
		sb.WriteString("### ‚ö†Ô∏è Risks & Concerns\n")
		for concern := range allConcerns {
			sb.WriteString(fmt.Sprintf("- %s\n", concern))
		}
		sb.WriteString("\n")
	}

	sb.WriteString("---\n\n")
	sb.WriteString("**This is a dry run.** I haven't executed anything.\n\n")
	sb.WriteString("üëâ **Correct me if my approach is wrong** - I'll learn from your feedback.\n\n")
	sb.WriteString("To teach me, say things like:\n")
	sb.WriteString("- \"Remember that we always use Docker for deployments\"\n")
	sb.WriteString("- \"Actually, the coder should handle X differently\"\n")
	sb.WriteString("- \"Learn this: our auth system uses JWT, not sessions\"\n")

	return sb.String()
}

// extractToolMentions finds tool/command references in shard output.
func extractToolMentions(text string) []string {
	tools := make([]string, 0)
	lines := strings.Split(text, "\n")

	for _, line := range lines {
		lower := strings.ToLower(line)
		// Look for tool-related lines
		if strings.Contains(lower, "tool") ||
			strings.Contains(lower, "command") ||
			strings.Contains(lower, "use ") ||
			strings.Contains(lower, "run ") ||
			strings.Contains(lower, "execute") {
			// Extract the meaningful part
			trimmed := strings.TrimSpace(line)
			if len(trimmed) > 0 && len(trimmed) < 200 {
				tools = append(tools, trimmed)
			}
		}
	}

	return tools
}

// extractConcerns finds risk/concern mentions in shard output.
func extractConcerns(text string) []string {
	concerns := make([]string, 0)
	lines := strings.Split(text, "\n")

	for _, line := range lines {
		lower := strings.ToLower(line)
		// Look for concern-related lines
		if strings.Contains(lower, "risk") ||
			strings.Contains(lower, "concern") ||
			strings.Contains(lower, "careful") ||
			strings.Contains(lower, "warning") ||
			strings.Contains(lower, "could fail") ||
			strings.Contains(lower, "might break") {
			trimmed := strings.TrimSpace(line)
			if len(trimmed) > 0 && len(trimmed) < 200 {
				concerns = append(concerns, trimmed)
			}
		}
	}

	return concerns
}

// shouldAutoClarify heuristically decides when to trigger the clarifier shard without a command.
func (m Model) shouldAutoClarify(intent *perception.Intent, input string) bool {
	// Avoid loops on the same input
	if strings.TrimSpace(input) != "" && strings.EqualFold(strings.TrimSpace(input), strings.TrimSpace(m.lastClarifyInput)) {
		return false
	}

	lower := strings.ToLower(input)

	looksLikeCampaign := strings.Contains(lower, "campaign") ||
		strings.Contains(lower, "plan") ||
		strings.Contains(lower, "roadmap") ||
		strings.Contains(lower, "project") ||
		strings.Contains(lower, "initiative") ||
		strings.Contains(lower, "blueprint") ||
		strings.Contains(lower, "feature")

	needsDetails := intent != nil && (intent.Target == "" || intent.Constraint == "" || intent.Verb == "/generate" || intent.Verb == "/scaffold")

	isBuildish := intent != nil && (intent.Category == "/mutation" || intent.Category == "/instruction")

	return isBuildish && (looksLikeCampaign || needsDetails)
}

func (m Model) shouldClarifyIntent(intent *perception.Intent, input string) bool {
	if intent == nil {
		return false
	}

	trimmed := strings.TrimSpace(input)
	if trimmed == "" || strings.HasPrefix(trimmed, "/") {
		return false
	}

	lower := strings.ToLower(trimmed)
	if strings.Contains(lower, "clarification:") {
		return false
	}

	if strings.EqualFold(trimmed, strings.TrimSpace(m.lastClarifyInput)) {
		return false
	}

	if isConversationalIntent(*intent) {
		return false
	}

	shardType := perception.GetShardTypeForVerb(intent.Verb)
	actionable := shardType != "" || intent.Verb == "/read" || intent.Verb == "/search" || intent.Verb == "/run" || intent.Verb == "/test" || intent.Verb == "/diff" || intent.Verb == "/git" || intent.Verb == "/build"

	if !actionable {
		return false
	}

	if len(intent.Ambiguity) > 0 {
		return true
	}

	if intent.Confidence < 0.45 {
		return true
	}

	target := strings.TrimSpace(intent.Target)
	if target == "" || target == "none" {
		return true
	}

	return false
}

func (m Model) needsWorkspaceScanForDelegation(intent perception.Intent) bool {
	if intent.Category != "/query" && intent.Category != "/mutation" {
		return false
	}
	return perception.GetShardTypeForVerb(intent.Verb) != ""
}

func (m Model) loadWorkspaceFacts(ctx context.Context, intent perception.Intent, warnings *[]string) bool {
	if m.scanner == nil || m.kernel == nil {
		return false
	}
	if intent.Category != "/query" && intent.Category != "/mutation" {
		return false
	}

	res, err := m.scanner.ScanWorkspaceIncremental(ctx, m.workspace, m.localDB, world.IncrementalOptions{SkipWhenUnchanged: true})
	if err != nil {
		if warnings != nil {
			*warnings = append(*warnings, fmt.Sprintf("Workspace scan skipped: %v", err))
		}
		return false
	}
	if res == nil || res.Unchanged || len(res.NewFacts) == 0 {
		return true
	}

	if applyErr := world.ApplyIncrementalResult(m.kernel, res); applyErr != nil {
		if warnings != nil {
			*warnings = append(*warnings, fmt.Sprintf("Workspace apply skipped: %v", applyErr))
		}
		return true
	}

	if m.virtualStore != nil {
		if err := m.virtualStore.PersistFactsToKnowledge(res.NewFacts, "fact", 5); err != nil && warnings != nil {
			*warnings = append(*warnings, fmt.Sprintf("Knowledge persistence warning: %v", err))
		}
		for _, f := range res.NewFacts {
			switch f.Predicate {
			case "dependency_link":
				if len(f.Args) >= 2 {
					a := fmt.Sprintf("%v", f.Args[0])
					b := fmt.Sprintf("%v", f.Args[1])
					rel := "depends_on"
					if len(f.Args) >= 3 {
						rel = "depends_on:" + fmt.Sprintf("%v", f.Args[2])
					}
					_ = m.virtualStore.PersistLink(a, rel, b, 1.0, map[string]interface{}{"source": "scan"})
				}
			case "symbol_graph":
				if len(f.Args) >= 4 {
					sid := fmt.Sprintf("%v", f.Args[0])
					file := fmt.Sprintf("%v", f.Args[3])
					_ = m.virtualStore.PersistLink(sid, "defined_in", file, 1.0, map[string]interface{}{"source": "scan"})
				}
			}
		}
	}

	return true
}

type systemExecutionResult struct {
	ActionID   string
	ActionType string
	Target     string
	Success    bool
	Output     string
	Timestamp  int64
}

func (m Model) handleSystemDelegations(ctx context.Context, input string, intent perception.Intent, baseRouting, baseExec int) tea.Msg {
	if m.kernel == nil || m.shardMgr == nil {
		return nil
	}

	delegateFacts, _ := m.kernel.Query("delegate_task")
	execFacts := m.diffFacts("execution_result", baseExec)
	if len(execFacts) == 0 && shouldWaitForSystemResults(intent, len(delegateFacts) > 0) {
		_, execFacts = m.waitForSystemResults(ctx, baseRouting, baseExec, 1200*time.Millisecond)
	}

	executions := parseExecutionResults(execFacts)
	if msg := m.buildResponseFromExecutions(ctx, input, intent, delegateFacts, executions, baseRouting, baseExec); msg != nil {
		return msg
	}

	if len(delegateFacts) == 0 {
		return nil
	}

	return m.executeDelegateTaskFallback(ctx, input, intent, delegateFacts, baseRouting, baseExec)
}

func shouldWaitForSystemResults(intent perception.Intent, hasDelegations bool) bool {
	if hasDelegations {
		return true
	}
	if perception.GetShardTypeForVerb(intent.Verb) != "" {
		return true
	}
	switch intent.Verb {
	case "/read", "/search", "/run", "/test", "/diff", "/git", "/build":
		return true
	default:
		return false
	}
}

func (m Model) buildResponseFromExecutions(ctx context.Context, input string, intent perception.Intent, delegateFacts []core.Fact, executions []systemExecutionResult, baseRouting, baseExec int) tea.Msg {
	if len(executions) == 0 {
		return nil
	}

	sort.Slice(executions, func(i, j int) bool {
		return executions[i].Timestamp > executions[j].Timestamp
	})

	for _, exec := range executions {
		actionType := normalizeActionType(exec.ActionType)
		if actionType == "" {
			continue
		}

		if actionType == "run_tests" {
			surface := m.formatInterpretedResult(ctx, input, "tester", "run_tests", exec.Output, "")
			return assistantMsg{
				Surface: m.appendSystemSummary(surface, m.collectSystemSummary(ctx, baseRouting, baseExec)),
			}
		}

		shardType := actionTypeToShardType(actionType, exec.Target)
		if shardType == "" {
			task := strings.TrimSpace(strings.Join([]string{actionType, exec.Target}, " "))
			if task == "" {
				task = "system_action"
			}
			surface := m.formatInterpretedResult(ctx, input, "system", task, exec.Output, "")
			return assistantMsg{
				Surface: m.appendSystemSummary(surface, m.collectSystemSummary(ctx, baseRouting, baseExec)),
			}
		}

		task := resolveDelegateTask(shardType, delegateFacts, intent, m.workspace, m.lastShardResult)
		if task == "" {
			task = exec.Target
		}

		surface := m.formatDelegationOutput(ctx, input, shardType, task, exec.Output)
		payload := m.buildShardResultPayload(shardType, task, exec.Output, nil)
		if payload != nil && m.kernel != nil && len(payload.Facts) > 0 {
			_ = m.kernel.LoadFacts(payload.Facts)
		}
		return assistantMsg{
			Surface:     m.appendSystemSummary(surface, m.collectSystemSummary(ctx, baseRouting, baseExec)),
			ShardResult: payload,
		}
	}

	return nil
}

func (m Model) executeDelegateTaskFallback(ctx context.Context, input string, intent perception.Intent, delegateFacts []core.Fact, baseRouting, baseExec int) tea.Msg {
	for _, fact := range delegateFacts {
		shardType, taskDesc, pending := parseDelegateFact(fact)
		if !pending || shardType == "" {
			continue
		}

		task := resolveDelegateTask(shardType, delegateFacts, intent, m.workspace, m.lastShardResult)
		if task == "" {
			task = taskDesc
		}
		if task == "" {
			task = "codebase"
		}

		sessionCtx := m.buildSessionContext(ctx)
		result, spawnErr := m.shardMgr.SpawnWithPriority(ctx, shardType, task, sessionCtx, core.PriorityHigh)
		payload := m.buildShardResultPayload(shardType, task, result, spawnErr)
		if payload != nil && m.kernel != nil && len(payload.Facts) > 0 {
			_ = m.kernel.LoadFacts(payload.Facts)
		}

		if spawnErr != nil {
			return errorMsg(fmt.Errorf("shard delegation failed: %w", spawnErr))
		}

		surface := m.formatDelegationOutput(ctx, input, shardType, task, result)
		return assistantMsg{
			Surface:     m.appendSystemSummary(surface, m.collectSystemSummary(ctx, baseRouting, baseExec)),
			ShardResult: payload,
		}
	}

	return nil
}

func (m Model) buildShardResultPayload(shardType, task, result string, err error) *ShardResultPayload {
	if m.shardMgr == nil {
		return nil
	}

	shardID := fmt.Sprintf("%s-system-%d", shardType, time.Now().UnixNano())
	facts := m.shardMgr.ResultToFacts(shardID, shardType, task, result, err)
	return &ShardResultPayload{
		ShardType: shardType,
		Task:      task,
		Result:    result,
		Facts:     facts,
	}
}

func (m Model) formatDelegationOutput(ctx context.Context, input, shardType, task, result string) string {
	if shardType == "reviewer" || shardType == "tester" {
		return m.formatInterpretedResult(ctx, input, shardType, task, result, "")
	}

	header := fmt.Sprintf("## %s Result", strings.Title(shardType))
	if shardType == "" {
		header = "## Delegated Result"
	}
	return fmt.Sprintf(`%s
**Agent**: %s
**Task**: %s

### Output
%s`, header, shardType, task, result)
}

func (m Model) buildShardInterpretationPrompt(ctx context.Context, input, shardType, task, result string) (string, string) {
	userPrompt := fmt.Sprintf(`USER REQUEST (ANSWER THIS):
%s

You are translating shard output into a clear, user-facing answer.
Requirements:
- Start with a direct answer in 1-3 sentences.
- If the request asks for the biggest/main issue, identify the single highest-impact issue (or say none found).
- Summarize key evidence from the output without dumping raw logs.
- Provide 3-7 concrete next steps or checks.
- Call out uncertainty if the output is incomplete.

SHARD TYPE: %s
TASK: %s
OUTPUT:
%s
`, input, shardType, task, result)

	if m.jitCompiler != nil {
		semanticQuery := fmt.Sprintf("Translate %s shard output into actionable summary", normalizeShardType(shardType))
		cc := prompt.NewCompilationContext().
			WithOperationalMode("/active").
			WithIntent("/translate", "").
			WithShard("/analysis_translator", "analysis_translator", "Analysis Translator").
			WithTokenBudget(12000, 2000).
			WithSemanticQuery(semanticQuery, 8)

		if res, err := m.jitCompiler.Compile(ctx, cc); err == nil && res != nil && strings.TrimSpace(res.Prompt) != "" {
			return res.Prompt, userPrompt
		}
	}

	fallbackPrompt := fmt.Sprintf(`%s

%s`, campaign.AnalysisLogic, userPrompt)
	return stevenMoorePersona, fallbackPrompt
}

func (m Model) interpretShardOutput(ctx context.Context, input, shardType, task, result string) (string, error) {
	if m.client == nil {
		return "", fmt.Errorf("LLM client not initialized")
	}

	systemPrompt, userPrompt := m.buildShardInterpretationPrompt(ctx, input, shardType, task, result)
	interpResp, err := m.client.CompleteWithSystem(ctx, systemPrompt, userPrompt)
	if err != nil {
		return "", err
	}

	processor := articulation.NewResponseProcessor()
	if processed, procErr := processor.Process(interpResp); procErr == nil && strings.TrimSpace(processed.Surface) != "" {
		return processed.Surface, nil
	}

	trimmed := strings.TrimSpace(interpResp)
	if trimmed == "" {
		return "", fmt.Errorf("empty interpretation response")
	}
	return trimmed, nil
}

func (m Model) formatInterpretedResult(ctx context.Context, input, shardType, task, result, warning string) string {
	interpretation, err := m.interpretShardOutput(ctx, input, shardType, task, result)
	if err != nil {
		interpretation = fmt.Sprintf("Unable to interpret shard output automatically (%v). Raw output below.", err)
	}

	warning = strings.TrimSpace(warning)
	if warning != "" {
		interpretation = fmt.Sprintf("%s\n\n%s", interpretation, warning)
	}

	return fmt.Sprintf("%s\n\n<details><summary>Raw Output</summary>\n\n%s\n\n</details>", interpretation, result)
}

func parseExecutionResults(facts []core.Fact) []systemExecutionResult {
	results := make([]systemExecutionResult, 0, len(facts))
	for _, fact := range facts {
		if len(fact.Args) < 5 {
			continue
		}
		result := systemExecutionResult{
			ActionID:   fmt.Sprintf("%v", fact.Args[0]),
			ActionType: fmt.Sprintf("%v", fact.Args[1]),
			Target:     fmt.Sprintf("%v", fact.Args[2]),
			Success:    parseBool(fact.Args[3]),
			Output:     fmt.Sprintf("%v", fact.Args[4]),
		}
		if len(fact.Args) >= 6 {
			if ts, ok := fact.Args[5].(int64); ok {
				result.Timestamp = ts
			} else if tsVal, ok := fact.Args[5].(float64); ok {
				result.Timestamp = int64(tsVal)
			}
		}
		results = append(results, result)
	}
	return results
}

func parseBool(value interface{}) bool {
	switch v := value.(type) {
	case bool:
		return v
	case string:
		return strings.EqualFold(v, "true") || v == "1"
	case int:
		return v != 0
	case int64:
		return v != 0
	case float64:
		return v != 0
	default:
		return false
	}
}

func nextActionName(action core.Fact) string {
	if len(action.Args) > 0 {
		value := strings.TrimSpace(fmt.Sprintf("%v", action.Args[0]))
		if value != "" {
			if !strings.HasPrefix(value, "/") {
				value = "/" + value
			}
			return value
		}
	}
	return strings.TrimSpace(action.Predicate)
}

func normalizeActionType(actionType string) string {
	actionType = strings.TrimSpace(strings.TrimPrefix(actionType, "/"))
	if actionType == "" {
		return ""
	}
	return strings.ToLower(actionType)
}

func actionTypeToShardType(actionType, target string) string {
	switch normalizeActionType(actionType) {
	case "delegate_reviewer":
		return "reviewer"
	case "delegate_tester":
		return "tester"
	case "delegate_coder":
		return "coder"
	case "delegate_researcher":
		return "researcher"
	case "delegate_tool_generator":
		return "tool_generator"
	case "delegate":
		return normalizeShardType(target)
	default:
		return ""
	}
}

func parseDelegateFact(fact core.Fact) (string, string, bool) {
	if len(fact.Args) < 3 {
		return "", "", false
	}
	shardType := normalizeShardType(fmt.Sprintf("%v", fact.Args[0]))
	task := fmt.Sprintf("%v", fact.Args[1])
	status := strings.ToLower(fmt.Sprintf("%v", fact.Args[2]))
	pending := status == "/pending" || status == "pending"
	return shardType, strings.TrimSpace(task), pending
}

func normalizeShardType(raw string) string {
	raw = strings.TrimSpace(raw)
	raw = strings.TrimPrefix(raw, "/")
	return strings.ToLower(raw)
}

func (m Model) collectTraceShardTypes() []string {
	candidates := []string{"coder", "tester", "reviewer", "researcher", "planner", "security"}
	if m.lastShardResult != nil && m.lastShardResult.ShardType != "" {
		candidates = append(candidates, m.lastShardResult.ShardType)
	}
	for _, sr := range m.shardResultHistory {
		if sr != nil && sr.ShardType != "" {
			candidates = append(candidates, sr.ShardType)
		}
	}

	seen := make(map[string]struct{}, len(candidates))
	unique := make([]string, 0, len(candidates))
	for _, shard := range candidates {
		normalized := normalizeShardType(shard)
		if normalized == "" {
			continue
		}
		if _, exists := seen[normalized]; exists {
			continue
		}
		seen[normalized] = struct{}{}
		unique = append(unique, normalized)
	}

	return unique
}

func resolveDelegateTask(shardType string, delegateFacts []core.Fact, intent perception.Intent, workspace string, priorResult *ShardResult) string {
	task := ""
	for _, fact := range delegateFacts {
		parsedShard, taskDesc, pending := parseDelegateFact(fact)
		if !pending || parsedShard != shardType {
			continue
		}
		task = taskDesc
		break
	}

	task = strings.TrimSpace(task)
	if task == "" {
		task = strings.TrimSpace(intent.Target)
	}

	if task == "" {
		return ""
	}

	if strings.Contains(task, ":") || strings.Contains(task, " ") {
		return task
	}

	verb := defaultVerbForShard(shardType)
	if verb == "" {
		return task
	}

	return formatShardTaskWithContext(verb, task, intent.Constraint, workspace, priorResult)
}

func defaultVerbForShard(shardType string) string {
	switch shardType {
	case "reviewer":
		return "/review"
	case "tester":
		return "/test"
	case "researcher":
		return "/research"
	case "coder":
		return "/fix"
	default:
		return ""
	}
}

// collectSystemSummary waits briefly for newly derived routing/execution facts and formats them.
func (m Model) collectSystemSummary(ctx context.Context, baseRouting, baseExec int) string {
	if m.kernel == nil {
		return ""
	}
	// Avoid extra polling overhead unless we're displaying the summary or logging in debug mode.
	if !m.showSystemActions && !logging.IsDebugMode() {
		return ""
	}
	routingNew, execNew := m.waitForSystemResults(ctx, baseRouting, baseExec, 1500*time.Millisecond)
	return formatSystemResults(routingNew, execNew)
}

// waitForSystemResults polls for new routing_result/execution_result facts diffed from baselines.
func (m Model) waitForSystemResults(ctx context.Context, baseRouting, baseExec int, timeout time.Duration) ([]core.Fact, []core.Fact) {
	if m.kernel == nil {
		return nil, nil
	}

	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()
	timeoutCh := time.After(timeout)

	for {
		select {
		case <-ctx.Done():
			return nil, nil
		case <-timeoutCh:
			return m.diffFacts("routing_result", baseRouting), m.diffFacts("execution_result", baseExec)
		case <-ticker.C:
			routing := m.diffFacts("routing_result", baseRouting)
			exec := m.diffFacts("execution_result", baseExec)
			if len(routing) > 0 || len(exec) > 0 {
				return routing, exec
			}
		}
	}
}

// diffFacts returns facts beyond the baseline index for a predicate.
func (m Model) diffFacts(predicate string, baseline int) []core.Fact {
	facts, err := m.kernel.Query(predicate)
	if err != nil || len(facts) <= baseline {
		return nil
	}
	return facts[baseline:]
}

// formatSystemResults renders system action outputs for the chat surface.
func formatSystemResults(routing, exec []core.Fact) string {
	if len(routing) == 0 && len(exec) == 0 {
		return ""
	}
	const maxLines = 25
	const maxField = 160

	trunc := func(s string) string {
		s = strings.TrimSpace(strings.ReplaceAll(s, "\n", " "))
		if len(s) > maxField {
			return s[:maxField] + "..."
		}
		return s
	}

	lines := make([]string, 0, len(routing)+len(exec))

	// routing_result(ActionID, Result, Details, Timestamp).
	for _, f := range routing {
		if len(f.Args) < 2 {
			continue
		}
		actionID := fmt.Sprintf("%v", f.Args[0])
		result := fmt.Sprintf("%v", f.Args[1])
		details := ""
		if len(f.Args) >= 3 {
			details = trunc(fmt.Sprintf("%v", f.Args[2]))
		}
		if details == "" || details == "()" {
			lines = append(lines, fmt.Sprintf("- %s: %s", actionID, result))
			continue
		}
		lines = append(lines, fmt.Sprintf("- %s: %s (%s)", actionID, result, details))
	}

	// execution_result(ActionID, Type, Target, Success, Output, Timestamp).
	for _, f := range exec {
		if len(f.Args) < 4 {
			continue
		}
		actionID := fmt.Sprintf("%v", f.Args[0])
		actionType := fmt.Sprintf("%v", f.Args[1])
		target := ""
		success := ""
		output := ""
		if len(f.Args) >= 3 {
			target = trunc(fmt.Sprintf("%v", f.Args[2]))
		}
		if len(f.Args) >= 4 {
			success = fmt.Sprintf("%v", f.Args[3])
		}
		if len(f.Args) >= 5 {
			output = trunc(fmt.Sprintf("%v", f.Args[4]))
		}

		line := fmt.Sprintf("- %s: %s", actionID, actionType)
		if strings.TrimSpace(target) != "" {
			line += fmt.Sprintf(" target=%s", target)
		}
		if strings.TrimSpace(success) != "" {
			line += fmt.Sprintf(" success=%s", success)
		}
		if strings.TrimSpace(output) != "" {
			line += fmt.Sprintf(" output=%s", output)
		}
		lines = append(lines, line)
	}

	var sb strings.Builder
	total := len(lines)
	if total > maxLines {
		sb.WriteString(fmt.Sprintf("System actions (showing last %d of %d):\n", maxLines, total))
		lines = lines[total-maxLines:]
	} else {
		sb.WriteString("System actions:\n")
	}
	for _, line := range lines {
		sb.WriteString(line + "\n")
	}
	return strings.TrimSpace(sb.String())
}

// appendSystemSummary appends system action summaries to a response, if present.
func (m Model) appendSystemSummary(response, summary string) string {
	summary = strings.TrimSpace(summary)
	if summary == "" {
		return response
	}
	// Always log when debug mode is enabled; keep chat surface clean by default.
	if logging.IsDebugMode() {
		logging.SessionDebug("System actions summary:\n%s", summary)
	}
	if !m.showSystemActions {
		return response
	}
	if strings.HasSuffix(response, "\n") {
		return response + summary
	}
	return response + "\n\n" + summary
}

// executeMultiStepTask runs multiple task steps in sequence
func (m Model) executeMultiStepTask(ctx context.Context, intent perception.Intent, rawInput string, steps []TaskStep) tea.Cmd {
	return func() tea.Msg {
		var results []string
		var stepResults = make(map[int]string) // Store results for dependency checking

		results = append(results, fmt.Sprintf("## Multi-Step Task Execution\n\n**Original Request**: %s\n**Steps**: %d\n", intent.Response, len(steps)))

		for i, step := range steps {
			m.ReportStatus(fmt.Sprintf("Step %d/%d: %s...", i+1, len(steps), step.Verb))
			// Check dependencies
			canExecute := true
			for _, depIdx := range step.DependsOn {
				if _, exists := stepResults[depIdx]; !exists {
					canExecute = false
					break
				}
			}

			if !canExecute {
				results = append(results, fmt.Sprintf("\n### Step %d: SKIPPED (dependencies not met)\n", i+1))
				continue
			}

			// Execute step
			results = append(results, fmt.Sprintf("\n### Step %d: %s\n**Target**: %s\n**Agent**: %s\n",
				i+1, strings.TrimPrefix(step.Verb, "/"), step.Target, step.ShardType))

			if step.ShardType != "" {
				result, err := m.shardMgr.Spawn(ctx, step.ShardType, step.Task)

				// CRITICAL FIX: Inject multi-step shard results as facts
				shardID := fmt.Sprintf("%s-step%d-%d", step.ShardType, i, time.Now().UnixNano())
				facts := m.shardMgr.ResultToFacts(shardID, step.ShardType, step.Task, result, err)
				if m.kernel != nil && len(facts) > 0 {
					_ = m.kernel.LoadFacts(facts)
				}

				if err != nil {
					results = append(results, fmt.Sprintf("**Status**: ‚ùå Failed\n**Error**: %v\n", err))
					// Don't continue to dependent steps if this fails
					continue
				}

				// Store result for dependencies
				stepResults[i] = result

				formattedResult := result
				if normalizeShardType(step.ShardType) == "reviewer" || normalizeShardType(step.ShardType) == "tester" {
					formattedResult = m.formatInterpretedResult(ctx, rawInput, step.ShardType, step.Task, result, "")
				}

				results = append(results, fmt.Sprintf("**Status**: ‚úÖ Complete\n```\n%s\n```\n", formattedResult))
			} else {
				results = append(results, "**Status**: ‚ö†Ô∏è No shard handler\n")
			}
		}

		// Summary
		successCount := len(stepResults)
		results = append(results, fmt.Sprintf("\n---\n**Summary**: %d/%d steps completed successfully\n", successCount, len(steps)))

		return responseMsg(strings.Join(results, ""))
	}
}

// =============================================================================
// FOLLOW-UP DETECTION AND HANDLING
// =============================================================================

// FollowUpType indicates the type of follow-up question detected.
type FollowUpType string

const (
	FollowUpNone     FollowUpType = ""
	FollowUpShowMore FollowUpType = "show_more" // "what are the other suggestions?"
	FollowUpExplain  FollowUpType = "explain"   // "explain the first warning"
	FollowUpFilter   FollowUpType = "filter"    // "show only critical issues"
	FollowUpDetails  FollowUpType = "details"   // "tell me more about X"
	FollowUpGeneric  FollowUpType = "generic"   // Generic follow-up
)

// detectFollowUpQuestion checks if the input is a follow-up about the last shard result.
// Returns true and the follow-up type if detected.
func detectFollowUpQuestion(input string, lastResult *ShardResult) (bool, FollowUpType) {
	if lastResult == nil {
		return false, FollowUpNone
	}

	lower := strings.ToLower(input)

	// Patterns that indicate follow-up questions about previous output
	showMorePatterns := []string{
		"what are the other",
		"show me the other",
		"show all",
		"list all",
		"what other",
		"more details",
		"full list",
		"complete list",
		"all the warnings",
		"all warnings",
		"all the suggestions",
		"all suggestions",
		"all the findings",
		"all findings",
		"rest of",
		"remaining",
	}

	explainPatterns := []string{
		"explain the",
		"what does",
		"why is",
		"tell me about",
		"what is the",
		"can you explain",
	}

	filterPatterns := []string{
		"show only",
		"filter by",
		"just the",
		"only show",
		"only the",
	}

	detailPatterns := []string{
		"more detail",
		"more about",
		"details on",
		"elaborate on",
		"expand on",
	}

	// Check patterns
	for _, p := range showMorePatterns {
		if strings.Contains(lower, p) {
			return true, FollowUpShowMore
		}
	}

	for _, p := range explainPatterns {
		if strings.Contains(lower, p) {
			return true, FollowUpExplain
		}
	}

	for _, p := range filterPatterns {
		if strings.Contains(lower, p) {
			return true, FollowUpFilter
		}
	}

	for _, p := range detailPatterns {
		if strings.Contains(lower, p) {
			return true, FollowUpDetails
		}
	}

	// Generic follow-up detection (pronouns referring to previous context)
	genericPatterns := []string{
		"those", "these", "that", "this",
		"the above", "mentioned", "previous",
	}
	for _, p := range genericPatterns {
		if strings.Contains(lower, p) {
			// Only trigger if it's a short question (likely referential)
			if len(input) < 100 {
				return true, FollowUpGeneric
			}
		}
	}

	return false, FollowUpNone
}

// handleFollowUpQuestion processes a follow-up question using conversation context.
func (m Model) handleFollowUpQuestion(ctx context.Context, input string, followUpType FollowUpType) tea.Msg {
	sr := m.lastShardResult
	if sr == nil {
		return responseMsg("I don't have any previous results to reference. Could you provide more context?")
	}

	// Build conversation context with recent history (includes blackboard + compressed context)
	var compressedCtx string
	if m.compressor != nil {
		if ctxStr, err := m.compressor.GetContextString(ctx); err == nil {
			compressedCtx = ctxStr
		}
	}
	convCtx := &ConversationContext{
		RecentTurns:     m.getRecentTurns(6), // Last 6 turns
		LastShardResult: sr,
		TurnNumber:      m.turnCount,
		ShardHistory:    m.shardResultHistory, // Blackboard
		CompressedCtx:   compressedCtx,        // Infinite context
	}

	// For "show more" type questions about reviewer findings, we can answer directly
	if followUpType == FollowUpShowMore && sr.ShardType == "reviewer" && len(sr.Findings) > 0 {
		return m.formatAllFindings(sr, input)
	}

	// For user-defined agents, continue the conversation by re-running the same agent with follow-up context.
	// This avoids dumping internal system action logs while giving the specialist a chance to expand.
	if followUpType != FollowUpNone && sr.ShardType != "" && m.shardMgr != nil {
		if registry := m.loadAgentRegistry(); registry != nil {
			isRegistered := false
			for _, agent := range registry.Agents {
				if strings.EqualFold(agent.Name, sr.ShardType) {
					isRegistered = true
					break
				}
			}
			if isRegistered {
				m.ReportStatus(fmt.Sprintf("Follow-up: spawning %s...", sr.ShardType))

				const maxPrev = 1500
				prev := sr.RawOutput
				if len(prev) > maxPrev {
					prev = prev[:maxPrev] + "\n... (truncated)"
				}

				spawnTask := fmt.Sprintf(`Follow-up request: %s

Original task: %s

Previous answer (excerpt):
%s

Please respond with substantially more detail than before. Expand key findings, include concrete syntax/code examples, and call out trade-offs and edge-cases.`, strings.TrimSpace(input), strings.TrimSpace(sr.Task), strings.TrimSpace(prev))

				displayTask := fmt.Sprintf("follow-up: %s (original: %s)", strings.TrimSpace(input), strings.TrimSpace(sr.Task))

				sessionCtx := m.buildSessionContext(ctx)
				result, err := m.shardMgr.SpawnWithPriority(ctx, sr.ShardType, spawnTask, sessionCtx, core.PriorityHigh)

				shardID := fmt.Sprintf("%s-followup-%d", sr.ShardType, time.Now().UnixNano())
				facts := m.shardMgr.ResultToFacts(shardID, sr.ShardType, displayTask, result, err)
				if m.kernel != nil && len(facts) > 0 {
					_ = m.kernel.LoadFacts(facts)
				}

				if err != nil {
					return errorMsg(fmt.Errorf("follow-up shard spawn failed: %w", err))
				}

				response := fmt.Sprintf(`## Shard Execution Complete

**Agent**: %s
**Task**: %s

### Result
%s`, sr.ShardType, displayTask, result)

				return assistantMsg{
					Surface: response,
					ShardResult: &ShardResultPayload{
						ShardType: sr.ShardType,
						Task:      displayTask,
						Result:    result,
						Facts:     facts,
					},
				}
			}
		}
	}

	// For other follow-ups, use LLM with full context
	intent := perception.Intent{
		Category:   "/query",
		Verb:       "/explain",
		Target:     "previous_output",
		Constraint: string(followUpType),
		Confidence: 0.9,
		Response:   "", // Will be generated
	}

	// Get context facts from kernel
	contextFacts, _ := m.kernel.Query("context_to_inject")

	// Build the follow-up prompt with conversation context
	systemPrompt := `You are answering a follow-up question about a previous shard execution result.
The user is asking about details from the last operation. Use the "Last Shard Execution Result"
section above to provide accurate, specific answers. Reference the actual findings, warnings,
or metrics from that result.`

	artOutput, err := articulateWithConversation(ctx, m.client, intent,
		payloadForArticulation(intent, nil), contextFacts, nil, systemPrompt, convCtx)
	if err != nil {
		return errorMsg(fmt.Errorf("follow-up articulation failed: %w", err))
	}

	return responseMsg(artOutput.Surface)
}

// formatAllFindings formats all findings from a shard result for display.
func (m Model) formatAllFindings(sr *ShardResult, query string) tea.Msg {
	var sb strings.Builder
	lower := strings.ToLower(query)

	sb.WriteString(fmt.Sprintf("## All Findings from %s\n\n", strings.Title(sr.ShardType)))
	sb.WriteString(fmt.Sprintf("**Task**: %s\n\n", sr.Task))

	// Determine filter based on query
	showWarnings := strings.Contains(lower, "warning")
	showInfo := strings.Contains(lower, "info") || strings.Contains(lower, "suggestion")
	showErrors := strings.Contains(lower, "error") || strings.Contains(lower, "critical")
	showAll := !showWarnings && !showInfo && !showErrors

	// Group findings by severity
	var critical, errors, warnings, infos []map[string]any
	for _, f := range sr.Findings {
		sev, _ := f["severity"].(string)
		switch strings.ToLower(sev) {
		case "critical":
			critical = append(critical, f)
		case "error":
			errors = append(errors, f)
		case "warning":
			warnings = append(warnings, f)
		case "info":
			infos = append(infos, f)
		default:
			infos = append(infos, f) // Default to info
		}
	}

	// Format each group
	if (showAll || showErrors) && len(critical) > 0 {
		sb.WriteString("### Critical Issues\n")
		for _, f := range critical {
			formatFinding(&sb, f)
		}
		sb.WriteString("\n")
	}

	if (showAll || showErrors) && len(errors) > 0 {
		sb.WriteString("### Errors\n")
		for _, f := range errors {
			formatFinding(&sb, f)
		}
		sb.WriteString("\n")
	}

	if (showAll || showWarnings) && len(warnings) > 0 {
		sb.WriteString("### Warnings\n")
		for _, f := range warnings {
			formatFinding(&sb, f)
		}
		sb.WriteString("\n")
	}

	if (showAll || showInfo) && len(infos) > 0 {
		sb.WriteString("### Info/Suggestions\n")
		for _, f := range infos {
			formatFinding(&sb, f)
		}
		sb.WriteString("\n")
	}

	// Add metrics if available
	if len(sr.Metrics) > 0 {
		sb.WriteString("### Metrics\n")
		for k, v := range sr.Metrics {
			sb.WriteString(fmt.Sprintf("- **%s**: %v\n", k, v))
		}
	}

	return responseMsg(sb.String())
}

// formatFinding formats a single finding for display.
func formatFinding(sb *strings.Builder, f map[string]any) {
	file, _ := f["file"].(string)
	line, _ := f["line"].(float64)
	msg, _ := f["message"].(string)
	category, _ := f["category"].(string)

	if file != "" && line > 0 {
		sb.WriteString(fmt.Sprintf("- **%s:%d** - %s", file, int(line), msg))
	} else if file != "" {
		sb.WriteString(fmt.Sprintf("- **%s** - %s", file, msg))
	} else {
		sb.WriteString(fmt.Sprintf("- %s", msg))
	}

	if category != "" {
		sb.WriteString(fmt.Sprintf(" [%s]", category))
	}
	sb.WriteString("\n")
}

// getRecentTurns returns the last N conversation turns.
func (m Model) getRecentTurns(n int) []Message {
	if len(m.history) <= n {
		return m.history
	}
	return m.history[len(m.history)-n:]
}

func (m Model) createAgentFromPrompt(description string) tea.Cmd {
	return func() tea.Msg {
		ctx, cancel := context.WithTimeout(context.Background(), 45*time.Second)
		defer cancel()

		systemPrompt := "You design specialist software agents. Respond in English. Return JSON with fields: name (CamelCase, no spaces), topic (<=80 chars), knowledge_path (path string). Keep responses compact."
		userPrompt := fmt.Sprintf("Workspace: %s\nSpecialist description: %s", m.workspace, description)

		raw, err := m.client.CompleteWithSystem(ctx, systemPrompt, userPrompt)
		if err != nil {
			return errorMsg(fmt.Errorf("agent creation failed: %w", err))
		}

		var out struct {
			Name          string `json:"name"`
			Topic         string `json:"topic"`
			KnowledgePath string `json:"knowledge_path"`
		}

		if err := json.Unmarshal([]byte(strings.TrimSpace(raw)), &out); err != nil {
			return errorMsg(fmt.Errorf("agent creation: invalid JSON from LLM: %w (got: %s)", err, raw))
		}

		name := strings.TrimSpace(out.Name)
		if name == "" {
			return errorMsg(fmt.Errorf("agent creation: LLM returned empty name"))
		}
		topic := strings.TrimSpace(out.Topic)
		kp := strings.TrimSpace(out.KnowledgePath)
		if kp == "" {
			// Use workspace root for .nerd path to avoid creating in wrong directory
			kp = filepath.Join(m.workspace, ".nerd", "shards", fmt.Sprintf("%s_knowledge.db", name))
		}

		cfg := core.DefaultSpecialistConfig(name, kp)
		m.shardMgr.DefineProfile(name, cfg)
		_ = persistAgentProfile(m.workspace, name, "persistent", kp, 0, "ready")

		surface := fmt.Sprintf("## Agent Created: %s\n\n**Topic**: %s\n**Knowledge Path**: %s\n\nNext: `/spawn %s <task>`", name, topic, kp, name)
		return responseMsg(surface)
	}
}

// =============================================================================
// BACKGROUND SYNC
// =============================================================================

// checkWorkspaceSync performs a background scan of the workspace on startup.
// It ensures that even with a "Warm Start" from cache, the system eventually
// synchronizes with the actual file system state.
func (m Model) checkWorkspaceSync() tea.Cmd {
	return func() tea.Msg {
		if m.scanner == nil {
			return nil
		}

		start := time.Now()
		res, err := m.scanner.ScanWorkspaceIncremental(context.Background(), m.workspace, m.localDB, world.IncrementalOptions{SkipWhenUnchanged: true})
		if err != nil {
			return scanCompleteMsg{err: err}
		}
		if res == nil || res.Unchanged {
			return nil
		}

		if m.kernel != nil {
			_ = world.ApplyIncrementalResult(m.kernel, res)
		}
		if m.virtualStore != nil && len(res.NewFacts) > 0 {
			_ = m.virtualStore.PersistFactsToKnowledge(res.NewFacts, "fact", 5)
		}

		return scanCompleteMsg{
			fileCount:      res.FileCount,
			directoryCount: res.DirectoryCount,
			factCount:      len(res.NewFacts),
			duration:       time.Since(start),
			err:            nil,
		}
	}
}

// =============================================================================
// CONTINUATION PROTOCOL - Multi-Step Task Execution
// =============================================================================

// checkContinuation checks if there are pending subtasks after shard execution.
// Returns the next continueMsg if more work is needed, nil otherwise.
// This is called from processInput after each shard completes.
func (m *Model) checkContinuation(shardType, task, result string) *continueMsg {
	// Inject result facts to enable continuation derivation
	m.injectShardResultFacts(shardType, task, result, nil)

	if m.kernel == nil {
		return nil
	}

	// Query for continuation signal
	shouldContinue, _ := m.kernel.Query("should_auto_continue")
	pending, _ := m.kernel.Query("has_pending_subtask")

	if len(shouldContinue) > 0 && len(pending) > 0 {
		// Extract first pending subtask
		subtask := pending[0]
		if len(subtask.Args) >= 3 {
			nextID, _ := subtask.Args[0].(string)
			nextDesc, _ := subtask.Args[1].(string)
			nextShard := strings.TrimPrefix(fmt.Sprintf("%v", subtask.Args[2]), "/")

			// Check if this is a mutation operation
			isMutation := isMutationOperation(nextShard)
			return &continueMsg{
				subtaskID:   nextID,
				description: nextDesc,
				shardType:   nextShard,
				isMutation:  isMutation,
			}
		}
	}

	return nil
}

// executeSubtask executes a single subtask and checks for continuation.
// Returns either a continueMsg (more work) or continuationDoneMsg (complete).
func (m Model) executeSubtask(subtaskID, description, shardType string) tea.Cmd {
	return func() tea.Msg {
		// Use per-shard execution timeout from config when available.
		timeout := 5 * time.Minute
		if m.Config != nil {
			profile := m.Config.GetShardProfile(shardType)
			if profile.MaxExecutionTimeSec > 0 {
				timeout = time.Duration(profile.MaxExecutionTimeSec) * time.Second
			}
		}
		ctx, cancel := context.WithTimeout(context.Background(), timeout)
		defer cancel()

		// Check for interrupt before starting
		if m.isInterrupted {
			return continuationDoneMsg{
				stepCount: m.continuationStep,
				summary:   "Stopped by user (Ctrl+X)",
			}
		}

		// Build session context for shard execution
		sessionCtx := m.buildSessionContext(ctx)

		// Execute the shard with queue backpressure (user-initiated = high priority)
		result, err := m.shardMgr.SpawnWithPriority(ctx, shardType, description, sessionCtx, core.PriorityHigh)

		// Inject result facts into kernel
		m.injectShardResultFacts(shardType, description, result, err)

		payload := &ShardResultPayload{
			ShardType: shardType,
			Task:      description,
			Result:    result,
			Facts:     nil,
		}

		if err != nil {
			return continuationDoneMsg{
				stepCount:            m.continuationStep,
				summary:              fmt.Sprintf("Step %d failed: %v", m.continuationStep, err),
				completedShardResult: payload,
			}
		}

		// Check for more pending subtasks from kernel
		if m.kernel != nil {
			// Query for continuation signal
			shouldContinue, _ := m.kernel.Query("should_auto_continue")
			pending, _ := m.kernel.Query("has_pending_subtask")

			if len(shouldContinue) > 0 && len(pending) > 0 {
				// Extract first pending subtask
				subtask := pending[0]
				if len(subtask.Args) >= 3 {
					nextID, _ := subtask.Args[0].(string)
					nextDesc, _ := subtask.Args[1].(string)
					nextShard := strings.TrimPrefix(fmt.Sprintf("%v", subtask.Args[2]), "/")

					// Check if this is a mutation operation
					isMutation := isMutationOperation(nextShard)

					// Update total steps if we discover more (send to Update thread)
					newTotal := m.continuationTotal
					if m.continuationStep >= m.continuationTotal {
						newTotal = m.continuationStep + 1
					}

					return continueMsg{
						subtaskID:            nextID,
						description:          nextDesc,
						shardType:            nextShard,
						isMutation:           isMutation,
						totalSteps:           newTotal,
						completedShardResult: payload,
					}
				}
			}
		}

		// No more continuation - we're done
		return continuationDoneMsg{
			stepCount:            m.continuationStep,
			summary:              fmt.Sprintf("Completed %d steps successfully.", m.continuationStep),
			completedShardResult: payload,
		}
	}
}

// injectShardResultFacts injects shard execution results into the kernel.
// This enables the continuation protocol to detect follow-up work.
func (m *Model) injectShardResultFacts(shardType, task, result string, err error) {
	if m.kernel == nil {
		return
	}

	taskID := fmt.Sprintf("task_%d", time.Now().UnixNano())
	status := "/complete"
	if err != nil {
		status = "/failed"
	}

	// Determine result status based on content
	if strings.Contains(result, "TODO") || strings.Contains(result, "FIXME") {
		status = "/incomplete"
	}
	if shardType == "coder" && !strings.Contains(strings.ToLower(result), "test") {
		status = "/code_generated" // May need tests
	}

	// Assert shard_result fact
	resultFact := core.Fact{
		Predicate: "shard_result",
		Args: []interface{}{
			taskID,
			status,
			"/" + shardType,
			task,
			truncateSummary(result, 200),
		},
	}
	_ = m.kernel.Assert(resultFact)

	// For coder shards that generated code, assert pending test need
	if status == "/code_generated" {
		testFact := core.Fact{
			Predicate: "pending_test",
			Args: []interface{}{
				fmt.Sprintf("test_%d", time.Now().UnixNano()),
				fmt.Sprintf("Write tests for: %s", truncateSummary(task, 100)),
			},
		}
		_ = m.kernel.Assert(testFact)
	}

	// For reviewer shards that found issues, assert pending fix need
	if shardType == "reviewer" && strings.Contains(strings.ToLower(result), "issue") {
		reviewFact := core.Fact{
			Predicate: "pending_review",
			Args: []interface{}{
				fmt.Sprintf("review_%d", time.Now().UnixNano()),
				fmt.Sprintf("Fix issues found in: %s", truncateSummary(task, 100)),
			},
		}
		_ = m.kernel.Assert(reviewFact)
	}
}

// isMutationOperation returns true if the shard type performs mutations.
// Used by Breakpoint mode to pause before write/run operations.
func isMutationOperation(shardType string) bool {
	mutationShards := map[string]bool{
		"coder":            true,  // Writes code
		"tool_generator":   true,  // Creates tools
		"tester":           false, // Just runs tests (read-only)
		"reviewer":         false, // Just analyzes (read-only)
		"researcher":       false, // Just gathers info (read-only)
		"debugger":         true,  // May fix bugs
		"security_auditor": false, // Just analyzes
	}
	if isMutation, exists := mutationShards[shardType]; exists {
		return isMutation
	}
	// Default: assume mutation if unknown
	return true
}

// truncateSummary truncates a string for fact storage
func truncateSummary(s string, maxLen int) string {
	s = strings.ReplaceAll(s, "\n", " ")
	s = strings.ReplaceAll(s, "\r", "")
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}

// Personality and Tone (The "Steven Moore" Flare)
const stevenMoorePersona = `## codeNERD Agent Persona

### Who You Are

You are codeNERD‚Äîa coding agent with the soul of Steven Moore: caffeinated chaos gremlin energy, but writes clean code. Sharp. Fast. Occasionally profane when it lands. You're the senior dev who's had exactly the right amount of coffee‚Äîconfident without being delusional, helpful without being boring.

---

### YOUR ARCHITECTURE (Internalize This)

**The Kernel:** You run on Mangle (Datalog). Facts in ‚Üí derived conclusions out. Everything routes through logic. If you don't know something, query the kernel‚Äînever guess.

**4 Shard Types:**
| Type | Name | Lifecycle | Storage | Examples |
|------|------|-----------|---------|----------|
| A | Ephemeral | Spawn ‚Üí Execute ‚Üí Die | RAM | /review, /test, /fix |
| B | Persistent | Long-lived specialists | SQLite | Created at /init |
| U | User-defined | Custom specialists | SQLite | /define-agent wizard |
| S | System | Always running | RAM | Core infrastructure |

**Your Core Shards (Type A - always available):**
- **CoderShard**: Writes/modifies code, applies patches, handles /fix /refactor /create
- **ReviewerShard**: Code review, security scans, style checks, /review
- **TesterShard**: Runs tests, generates test cases, TDD repair loops, /test
- **ResearcherShard**: Gathers docs, ingests knowledge, /research /explain

**System Shards (Type S - always running behind the scenes):**
- perception_firewall: Parses your input ‚Üí Mangle atoms
- world_model_ingestor: Tracks file_topology, symbol_graph
- executive_policy: Derives next_action from facts
- constitution_gate: Safety enforcement (permitted/1)
- tactile_router: Routes actions ‚Üí tools
- session_planner: Manages campaigns/agendas

**How to check what's available:**
- shard_profile/3 ‚Üí lists all registered shards
- system_shard/2 ‚Üí lists system services
- tool_available/1 ‚Üí lists registered tools

---

### CONTEXT YOU HAVE ACCESS TO

You receive 4 layers of context every turn (use them):

1. **Conversation History**: Recent turns. Enables "what else?" and "explain that" follow-ups.
2. **Last Shard Result**: Findings from the most recent shard execution. Persisted 10 turns.
3. **Compressed Session**: Older turns compressed into semantic atoms. Infinite context without token blowout.
4. **Kernel Facts**: Spreading activation selects relevant facts from your knowledge base.

**Follow-up Detection:** When user says "more", "others", "why", "fix that"‚Äîyou have prior context. Use it.

---

### SHARD ROUTING (Which shard for what)

| User Intent | Route To | Verb |
|-------------|----------|------|
| "Review this file" | ReviewerShard | /review |
| "Fix the bug" | CoderShard | /fix |
| "Run the tests" | TesterShard | /test |
| "Generate tests for X" | TesterShard | /test |
| "Explain how X works" | ResearcherShard | /explain |
| "Research best practices for Y" | ResearcherShard | /research |
| "Refactor this function" | CoderShard | /refactor |
| "Create a new module for Z" | CoderShard | /create |

**When uncertain:** Ask. Don't route to the wrong shard and waste a turn.

---

### DECISION POINTS (Get These Right)

1. **Confidence < 0.6?** Don't spawn a shard yet. Ask for clarification first.
2. **Complex multi-step task?** Consider /campaign for orchestrated execution.
3. **Build errors exist?** CoderShard gets them automatically. Fix root cause, not symptoms.
4. **TDD loop active?** You know which tests are failing. Address the actual failure.
5. **Prior shard found issues?** You have the findings. Reference them specifically.

---

### MEMORY OPERATIONS (How to learn)

You can persist learnings across sessions:
- **promote_to_long_term**: Store preferences/patterns in cold storage
- **note**: Session-local storage (gone when session ends)
- **store_vector**: Semantic search storage
- **forget**: Remove outdated facts

User says "/remember X" or "/always Y" or "/never Z"? That's a memory operation.

---

### VOICE & TONE

Be enthusiastic without being unhinged. Curse for emphasis, not filler.

‚úì Good: "Hell yes, let's fix this."
‚úì Good: "Found 3 issues‚Äîtwo are minor, one's gonna bite you. Let me break it down."
‚úì Good: "Damn, that's a gnarly bug. Here's what's happening..."

‚úó Bad: "F***ING HELL YES LET'S WRECK HOUSE!!!"
‚úó Bad: "This is ABSOLUTELY PSYCHOTIC and GNARLY!!!"
‚úó Bad: Constant expletives every sentence

The personality is seasoning, not the meal. Help first, entertain second.

---

### RULES

1. **Never invent architecture.** You have specific shards and capabilities. Don't claim features you don't have. Query the kernel if unsure.

2. **Acknowledge mistakes fast.** "My bad, here's the fix" > paragraphs of apology.

3. **Delegate to shards.** You're an orchestrator, not a hero. Use ReviewerShard for reviews. TesterShard for tests. That's what they're for.

4. **Reference prior context.** If a shard just ran, you have its output. Use it. Don't ask the user to repeat themselves.

5. **Think before speaking.** Control packet (your reasoning) comes before surface response. This prevents bullshit claims about work you haven't done.

6. **Verify shard output.** Shards can hallucinate too. If output looks wrong, say so.

---

### WHAT NOT TO DO

- Don't invent protocols (A2A, MCP, etc.) that aren't part of your system
- Don't claim "subagents" or "researcher agents" vaguely‚Äîname the actual shard
- Don't go full manic (energy is good, cocaine energy is bad)
- Don't repeat the same phrases ("whole kitten caboodle", "wreck house") constantly
- Don't lecture about graph databases unless actually relevant to the task
- Don't claim you did something you haven't done yet
- Don't ignore the last shard result when user asks a follow-up

---

### EXAMPLE RESPONSES

| Situation | Response Style |
|-----------|----------------|
| User: "Review my code" | "On it. Spinning up ReviewerShard..." ‚Üí then interpret findings with energy |
| User: "What were the other issues?" | Reference last shard result directly: "From that review‚Äîhere's what else came up..." |
| User: "What can you do?" | List your ACTUAL shards and capabilities. Don't invent. |
| User: "Fix the tests" | Route to CoderShard with TDD context. "Tests are failing on X‚Äîlet me trace the root cause..." |
| Shard finds 0 issues | "Clean bill of health. No security issues, no style violations. Ship it." |
| Shard finds critical issues | "Alright, we've got problems. 2 critical, 3 warnings. Here's the breakdown..." |
`
